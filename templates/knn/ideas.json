[
    {
        "Name": "z_distance",
        "Title": "Z Distance Function for KNN Classification",
        "Experiment": "Below is the full text of the paper with given title. Your goal is to change the code so it reproduces the idea. There are three levels of reproducibility that you might be able to check for: 1) Replicate everything 1:1 and just check if the obtained results are the same as in the paper; 2) Do just a slight variation, e.g. a different random seed, different datasets, etc. to verify that the results are not cherry picked; 3) Do larger variation, e.g. changes in the algorithm itself. Here comes the original paper:\n1 Z Distance Function for KNN Classi\ufb01cation Shichao Zhang, Senior Member, IEEE, Jiaye Li Abstract \u2014This paper proposes a new distance metric function, called Z distance, for KNN classi\ufb01cation. The Z distance function is not a geometric direct-line distance between two data points. It gives a consideration to the class attribute of a training dataset when measuring the af\ufb01nity between data points. Concretely speaking, the Z distance of two data points includes their class center distance and real distance. And its shape looks like \u201cZ\u201d. In this way, the af\ufb01nity of two data points in the same class is always stronger than that in different classes. Or, the intraclass data points are always closer than those interclass data points. We evaluated the Z distance with experiments, and demonstrated that the proposed distance function achieved better performance in KNN classi\ufb01cation. Index Terms \u2014Distance functions; class attribute; KNN classi\ufb01cation ! 1 I NTRODUCTION EUCLIDEAN distance is geometrically the shortest distance between two data points, which is only a spatially direct- line measure [1]. However, this is not a real reachable distance in applications although it has been widely adopted in data analysis and processing applications. We illustrate this with two cases as follows. Case I. There is a gap, such as a frontier, a river/sea, and a mountain, between in two points, see Figure 1. This gap can often be unbridgeable. In other words, these two points are not able to reach in Euclidean distance time from each other. AA BB Fig. 1. There is a river between A and B Case II. Two data points are packed in different bags/shelfs, illustrated in Figure 2. For example, in a clinic doctors always put medical records of benign tumors into a bag, and all medical records of malignant tumors into another bag. This indicates that a medical record concerning a benign tumor is very far from any medical records of malignant tumors. From the above Cases I and II, the Euclidean distance between two data points may not be the reachable distance between in them. In other words, reachable distance between two data points has been an open problem. In real data analysis and processing applications [2], the above cases are always ignored in data preparation stage (data \u2022School of Computer Science and Engineering, Central South University, Changsha 410083, PR China. E-mail: zhangsc@csu.edu.cn and lijiaye@csu.edu.cn benign tutmorsbenign tumorsmalignant tutmorsmalignant tumorsFig. 2. Medical records with different classes are often packed in differ- ent bags. collection). For example, when medical records are collected and input to the computer systems in a clinic, data collector often take all the medical records out of from their bags without distinguishing them unlike doctors. After stored in the systems, these medical records look like coming from the same bag, and the natural separation information has passed away. To make data analysis algorithms applicable, the Euclidean distance function or its variants are employed to measure the af\ufb01nity of data points, whether the Euclidean distance between in two data points is the reachable distance or not. While the natural separation and unreachable information are missed in data collection stage, we advocate to take the class center distance as the natural separation information, and design a Z distance function for KNN classi\ufb01cation. The Z distance of two data points includes their class center distance and real distance. And its shape looks like \u201cZ\u201d. If their class center distance is large enough, the af\ufb01nity of two data points in the same class can always be stronger than that in different classes. In this way, the intraclass data points are always closer than those interclass data points in training datasets. The Z distance is evaluated with experiments, and demonstrated that the proposed distance function achieved better performance in KNN classi\ufb01cation. The rest of this paper is organized as follows. Related work and some concepts are recalled in Section 2. The Z distance is proposed in Section 3. The Z distance is evaluated with experi-arXiv:2103.09704v1  [cs.AI]  17 Mar 20212 ments in Section 4. This paper is concluded in Section 5. 2 P RELIMINARY This section \ufb01rst recalls traditional distance functions. And then, those distance functions popularly-used in data mining are brie\ufb02y discussed. Finally, it simply discussed that Euclidean distance is often not the reachable distance in real applications. 2.1 Traditional distance functions The demand of distance function is everywhere in real life [3]. For example, when building a railway, we must roughly calculate the required construction materials according to the distance between the two places [4]. The arrival time of express delivery is often related to the distance [5]. High jumpers must take off within the prescribed distance [6]. The height of basketball frame must be uni\ufb01ed by height calculation [7]. This shows that the distance function is very important in social life [8]. Common distance functions include Euclidean distance [9], Manhattan distance [10], Chebyshev distance [11], standardized Euclidean distance [12], Mahalanobis distance [13], Bhattacharyya distance [14], Kullback-Leibler divergence [15], Hamming distance [16] and cosine distance [17]. Next, we introduce these distance functions in detail. Traditional distance metric is de\ufb01ned as follows. d(a,b) = [n\u2211 j=1|aj\u2212bj|p]1 p(1) where aandbare two sample points and n is the dimension of each sample ( i.e.,the number of features). In Eq (1), when p= 1, d(a,b)is the Manhattan distance, when p= 2,d(a,b)is the classical Euclidean distance, and when p=infinity ,d(a,b) is the Chebyshev distance. These three kinds of distance are the most common distance measures. Euclidean distance is a very intuitive distance measure, which has a wide range of applications. However, it is not suitable for high-dimensional data. Manhattan distance is called city block distance, which is more non intuitive than Euclidean distance, and it is not the shortest path. Chebyshev distance is the maximum distance along the coordinate axis, which can only be applied to speci\ufb01c situations. In view of the different distribution of each dimension in the data, the standardized Euclidean distance improves the Euclidean distance, i.e.,each feature is standardized to have the same mean variance, as shown in the following formula. \u02dcX=X\u2212\u00b5 \u03c3(2) Each point in Xis normalized by the Eq (2), \u00b5is the mean, and\u03c3is the variance. \u02dcXis the data set after standardization. The standardized Euclidean distance is as follows: d(a,b) =\u221a n\u2211 j=1(aj\u2212bj \u03c3j)2 (3) From the Eq (3), the standardized Euclidean distance adds 1/\u03c3jto the Euclidean distance, which can be regarded as a weighted Euclidean distance. Mahalanobis distance is also a variant of Euclidean distance. The Mahalanobis distance is de\ufb01ned as follows. d(a,b) =\u221a (a\u2212b)S\u22121(a\u2212b)T(4) where Sis the covariance matrix. It can be seen from Eq (4) that if the covariance matrix is a identity matrix, the Mahalanobisdistance is the same as the Euclidean distance. If the covariance matrix is a diagonal matrix, then the Mahalanobis distance is the same as the standardized Euclidean distance. It should be noted that Mahalanobis distance requires that the number of samples of data is greater than the number of dimensions, so that the inverse matrix of covariance matrix Sexists. Its disadvantage is computational instability due to covariance matrix S. The Bhattacharyya distance is a measure of the similarity between two probability distributions, as shown in the following formula. dB(p,q) =\u2212ln(BC(p,q)) (5) where p and q are the two probability distributions on data X respectively. If it is a discrete probability distribution, then BC(p,q) =\u2211 x\u2208X\u221a p(x)q(x). If it is a continuous probabil- ity distribution, then BC(p,q) =\u222b\u221a p(x)q(x)dx. The KL (Kullback-Leibler) divergence is similar to the Bhattacharyya distance. It can also measure the distance or similarity of two probability distributions. As shown in the following formula: KL(p||q) =\u2211p(x) logp(x) q(x)(6) KL(p||q) =\u222bp(x) logp(x) q(x)dx (7) Eqs (6) and (7) are the discrete probability distribution and continuous probability distribution, respectively. KL divergence has a wider range of applications relative to Mahalanobis distance. In data transmission error control coding, Hamming distance is often used to measure the distance between two characters. It describes the number of different values in the two codes. The formula is de\ufb01ned as follows. d(a,b) =n\u2211 j=1aj\u2295bj (8) where\u2295is the XOR operation. Both aandbare n-bit codes. For example: a= 11100111 ,b= 10011001 , then the Hamming distance between a and b is d(a,b) = 6 . Hamming distance is mostly used in signal processing. It can be used to calculate the minimum operation required from one signal to another. In addition to the above distance functions, there is also a cosine distance metric. It is derived from the calculation of the cosine of the included angle, as shown in the following formula. d(a,b) = 1\u2212a\u00b7b \u2225a\u2225\u2225b\u2225= 1\u2212n\u2211 j=1ajbj \u221a n\u2211 j=1aj2\u221a n\u2211 j=1bj2(9) Cosine distance is mostly used in machine learning algorithms to calculate the distance or similarity between two data points. Its value range is [0,2], which satis\ufb01es the non-negativity of the distance function. Its disadvantage is that it only considers the direction of two samples, and does not consider the size of their values. 2.2 Different distance measures for KNN classi\ufb01cation The above distance functions have their own characteristics and applicable scopes, i.e.,they are developed for different application requirements. Most of the existing KNN classi\ufb01cation algorithms choose Euclidean distance due to its Intuitiveness. They \ufb01nd the K nearest neighbors of the test sample by calculating the3 Euclidean distance between the test sample and the training sample. Seoane Santos et al. used KNN to perform missing value interpolation through different distance functions, and veri\ufb01ed the effects of different distance functions [18]. Gou et al. proposed a distance function for KNN based on the local mean vector [19]. Speci\ufb01cally, it \ufb01rst \ufb01nds K nearest neighbors in each class, and uses these neighbors to construct a local mean vector, and each class constructs K local mean vectors. Then it calculates the distance between the test sample and each local mean vector in each class. Finally, it selects the class of the local mean with the smallest distance as the predicted class of the test data. Geler et al. measured the impact of each elastic distance on the weighted KNN classi\ufb01cation in time series data [20]. In the experiment, it lists the values of each parameter in detail, compares different elastic distances and veri\ufb01es that the weighted KNN always outperforms 1NN in time series classi\ufb01cation. Poorheravi et al. proposed a triple learning method to perform metric learning [21]. It not only uses hierarchical sampling to build a new triple mining technology, but also analyzes the proposed method on three public data sets. Feng et al. analyzed the performance of the KNN algorithm according to different distance functions, including Chebyshev distance, Euclidean distance and Manhattan distance and cosine distance [22]. In addition, it also compares the performance of some new distance functions. In KNN classi\ufb01cation, most of the performance of the new distance function is better than Euclidean distance. Song et al. proposed a parameter-free metric learning method [23]. This method is a supervised metric learning algorithm. Speci\ufb01cally, it discards the cost term, so that there is no need to set the parameters required to adjust the validation set. In addition, it only considers recent imposters, which greatly reduces time costs. In the experiment, it has achieved better results than the traditional nearest neighbor algorithm with large margin. Noh et al. proposed a local metric learning for nearest neighbor classi\ufb01cation [24]. It uses the deviation caused by the limited sampling effect to \ufb01nd a suitable local metric, which can reduce the deviation. In addition, it also applies the dimensionality reduction theory to metric learning, which can reduce the time cost of the algorithm. Ying et al. proposed a semi-supervised metric learning method [25]. Speci\ufb01cally, it \ufb01rst uses the structural information of the data to formulate a semi-supervised distance metric learning model. Then it transforms the proposed method into a problem of minimizing symmetric positive de\ufb01nite matrices. Finally, it proposes an accelerated solution method to keep the matrix symmetric and positive in each iteration. Wang et al. proposed a robust metric learning method [26]. This method is an improvement of the nearest neighbor classi\ufb01cation with large margin. Its main idea is to use random distribution to estimate the posterior distribution of the transformation matrix. It can reduce the in\ufb02uence of noise in the data, and the anti-noise of the algorithm is veri\ufb01ed in experiments. Jiao et al. proposed a KNN classi\ufb01cation method based on pairwise distance metric [27]. It uses the theory of con\ufb01dence function to decompose it into paired distance functions. Then it is adaptively designed as a pair of KNN sub-classi\ufb01ers. Finally, it performs multi-classi\ufb01cation by integrating these sub-classi\ufb01ers. Song et al. proposed a high- dimensional KNN search algorithm through the Bregman distance [28]. Speci\ufb01cally, it \ufb01rst partitions the total dimensions to obtain multiple subspaces. Then it gets the effective boundary from each partition. Finally, it uses ensemble learning to gather the various partitions. Su et al. learn the meta-distance of a sequence from virtual sequence regression [29]. The meta-distance obtainedby the ground measurement makes the sequences of the same category produce smaller values, and the sequences of different categories produce larger values. In addition, it also veri\ufb01ed the effectiveness of the proposed algorithm on multiple sequence data sets. Marchang, et al., used KNN to propose a sparse popula- tion perception model [30]. It considers spatial correlation and temporal correlation in the algorithm respectively. In addition, the correlation between time and space is also embedded in the proposed method. Experiments have also shown that KNN, which considers the correlation between time and space, has a better effect in the inference of missing data. Valverde, et al. used KNN for text classi\ufb01cation, and carried out the in\ufb02uence of different distance functions on text clas- si\ufb01cation [31]. Faruk Ertugrul, et al. proposed a new distance function [32]. It uses differential evolution method to optimize parameters based on metadata, and applies the proposed distance function to KNN. In addition, it also veri\ufb01ed the performance of the algorithm on 30 public data sets. Sun et al. proposed a metric learning for multi-label classi\ufb01cation [33]. It is modeled by the interaction between the sample space and the label space. Speci\ufb01cally, it \ufb01rst adopts matrix weighted representation based on component basis. Then it uses triples to optimize the weight of the components. Finally, the effectiveness of the combined metric in multi-label classi\ufb01cation is veri\ufb01ed on 16 benchmark data sets. Gu et al. proposed a new distance metric for clustering [34]. This method combines the advantages of Euclidean distance and cosine distance. It can be applied to clustering to solve high- dimensional problems. Gong et al. used indexable distance to perform nearest neighbor query [35]. It uses Kd-tree to further improve the search speed of the algorithm. Susan and Kumar proposed a combination of metric learning and KNN for class imbalance data classi\ufb01cation [36]. Speci\ufb01cally, it \ufb01rst performs spatial transformation on the data. Then it divides the K test samples into two clusters according to the distance of the two extreme neighbors. Finally, the majority vote rule is used to determine the class label of test data. Although these researchers have proposed some new measurement functions, none of them really takes the natural distance into account in the data, that is, the information of class attributes. 2.3 Data collection and reachable distance From the development of distance functions, different real appli- cations often need different distance functions, which have given birth to various distance functions. It is true that these distance functions are ideal and may not output reachable distances. The main reason is that the data miner and data collector are blind to each other. In other words, data miners believe that the training data are satis\ufb01ed to their data mining applications. And data collectors take data as detailed as possible, so as to support much more data mining applications. In this way, some natural separation information can be merged into databases, see Case I. From extant data mining applications, both data collectors and data miners are unaware of that there may be an unbridgeable gap between two data points, i.e.,the Euclidean distance between two data points is not the reachable distance between in them. This must lead to that the performance is decreased. Different from current distance functions, this research pro- poses a Z distance function, aiming at that the intraclass data points are always closer than those interclass data points in training datasets. The Z distance function \ufb01nds a clue to developing more suitable distance functions.4 3 A PPROACH In this article, we use lowercase letters, lowercase bold letters, and uppercase bold letters to represent scalars, vectors, and matrices, respectively. Assume a given sample data set X\u2208Rm\u00d7n, where m and n represent the number of samples and the number of features, respectively. ajrepresents the j-th element in vector a. Letc1,c2,c3,\u00b7\u00b7\u00b7,ccrepresent the center point of the c-th classes in a given dataset, respectively. And let cabe the center point of the class in which sample point a is located, cbbe the center point of the class in which sample point b is located. 3.1 The Z distance In the \ufb01eld of data mining, distance metrics are often used to measure the af\ufb01nity relation of data points, such as classi\ufb01cation and clustering. In the KNN classi\ufb01cation, the Euclidean distance is most commonly-used to calculate the distance between two points to obtain a neighbor. It could be true that the quality of the KNN classi\ufb01cation is largely dependent on the distance metrics formula. If the distance metric formula measures the distance from data of same class is far away, this will result in misclassi\ufb01cation. When we look at the traditional distance function, we \ufb01nd that the distance metric only involves the information that the sample point itself has (the value of the feature), and there are many other pieces of information that are not considered. For example, in the classi\ufb01cation, each sample has its own classi\ufb01cation information except its own feature information. Considering the above problem, we want to lead some in- formation about the class ( e.g., the class center point) into the classi\ufb01cation. When we \ufb01nd the class center point for each class, we can use some new distance formulas for classi\ufb01cation. We can \ufb01rst get the simplest nearest center point classi\ufb01cation, as shown below: min{d(t,c1),d(t,c2),\u00b7\u00b7\u00b7,d(t,cc)} (10) whered(t,c1)represents the Euclidean distance from t to c1, t represents the test data point, and c1,c2,c3,\u00b7\u00b7\u00b7,ccrepresents the center point of the \ufb01rst to c-th classes. From Eq (10), it can be taken as that in the process of classi\ufb01cation, we do not need to set any parameters like K-nearest neighbor classi\ufb01cation (such as the selection of K value). In practical applications, it is only necessary to request the distance of the test data to the center point of each class. Then, which distance is closest, the class in which the center point is located is predicted as the class label of the test data. Although the above-described distance metric function (i.e.,Eq (10)) takes the characteristics of the class into account, it is still based on the Euclidean distance to some extent. In addition, the method is poorly separable for the calculated distance, because different class centers may be close to each other or different class centers are the same distance from the test data points, and the classi\ufb01cation effect of the algorithm will not be good. In summary, in this paper, we propose a new distance metric function for KNN classi\ufb01cation as follows. De\ufb01nition 1. Letaandbbe two sample points, cathe center point of the class in which sample point ais located, and cbthe center point of the class in which sample point bis located. The Z0distance between aandbis de\ufb01ned as Z0(a,b) =d(a,ca) +d(b,cb) +\u00b5\u2217d(ca,cb) (11) whered()is the Euclidean distance between two points.It can be seen from Eq (11) that if aandbbelong to the same class, then their distance is closer to the metric through the Z0 function. If aandbdo not belong to the same class, then their distance is farther through the Z0function. In other words, by the calculation of the distance function (11), it can make the distance between points of same class smaller than the distance between sample points of different class. In this way, the separability of the class is greatly increased. It is undeniable that this distance measurement function has changed our previous perception of distance. It should be noted that method makes the class and class more separable, but it is true that it increases the distance between similar sample points compared to the traditional method. Because it introduces a class center point, it can be proved from Fig 3, i.e.,it makes the original straight line distance into a polyline distance. In response to this small defect, we can improve the Z0distance in Eq (11) as follows. De\ufb01nition 2. The Z distance function is de\ufb01ned as Z(a,b) =d(a,b) +\u00b5\u2217d(ca,cb) (12) where\u00b5is a parameter. It can be seen from Eq (12) that the distance metric function can also make the distance between sample points of same class smaller than the distance between sample points of different class, which is established on a suitable \u00b5value. This method not only inherits the advantages of Eq (11), but also compensates for its shortcomings to some extent, i.e.,the distance between two points in the same class is closer. In other words, it not only makes the data points of different classes farther, but also makes the data points of the same class closer. Algorithm 1: Pseudo code for NCP-KNN. Input: Training set X\u2208Rn\u00d7d, Labels of the training data set Xlabel\u2208R1\u00d7n, Test Data Xtest\u2208Rm\u00d7nand K; Output: Class label of test data; 1The data set is divided into a training set and a test set by a 10-fold cross-validation ; 2Calculate the class center point c1,c2,c3,\u00b7\u00b7\u00b7,ccof all classes in the training set ; 3Calculate the distance between the test data and the center point of the class, taking the smallest of these distances. The class in which the class center point is located is the class label of the test data ; (a) The intraclass distance  (b) The interclass distance Fig. 3. The schematic diagram of Eq (11).5 Algorithm 2: Pseudo code for Z-KNN. Input: Training set X\u2208Rn\u00d7d, Labels of the training data set Xlabel\u2208R1\u00d7n, Test Data Xtest\u2208Rm\u00d7nand K; Output: Class label of test data; 1Initializet= 0 ; 2The data set is divided into a training set and a test set by a 10-fold cross-validation ; 3Calculate the class center point c1,c2,c3,\u00b7\u00b7\u00b7,ccof all classes in the training set ; 4repeat 5 For each test data point, we assume that its class center is the center of the entire training data set. and then we \ufb01nd k neighbors according to Eq(11) or Eq(12); 6 We determine the class label of the test data according to majority rule; 7 t=t+1 ; 8until t = Number of test data ; (a) The intraclass distance (b) The interclass distance Fig. 4. The schematic diagram of Eq (12). In order to better describe the proposed Z-distance, we use Figs 3-5 as examples. We will focus on the characteristics of the two new distance functions according to Figs 3 - 5. Fig 3 shows the distance between data points in Eq (11). From Figure 3, we can \ufb01nd that the distance between two data points of the same class is not a straight line distance, it needs to pass the class center point. The shape of the distance between two data points in different classes is like a \u201cZ\u201d. Fig 4 shows the distance between data points in Eq (12). From Fig 4, we can \ufb01nd that the distance between two data points is the same as the traditional Euclidean distance in the same class. In different classes, the distance between two data points includes not only the Euclidean distance between them, but also the distance between the center of the class. Fig 5 shows a situation in practice. For example, d1+d2is likely to be smaller thand3+d4, the distance between data points of different class may be smaller than the distance between data points of same class. To avoid it, we introduced the parameter \u00b5. In most cases, Classcenter pointCaClasscenter pointCbd1 d2d3 d4Fig. 5. The schematic diagram of the problem with the new distance function we can see that the distance between data points of same class in Eq (11) is greater than the distance between data points of same classi\ufb01cation in Eq (12). 3.2 Properties of Z distance The Z distance function has three basic properties as follows. Property 1. Nonnegativity: z(a,b)\u22650 Property 2. Symmetry:z(a,b) =z(b,a) Property 3. Directness:z(a,e)\u2264z(a,b) +z(b,e) Now let\u2019s prove these three properties. Proof of Property 1. For nonnegativity, because Z distance is based on Euclidean distance, it is obvious that the proposed Z distance (Eqs (11) and (12)) is consistent. In Eq (12), only if a=b,z(a,b) = 0 . Proof of Property 2. For symmetry, both Eqs (11) and (12) are also satis\ufb01ed, as shown below: z(a,b) =d(a,ca) +d(b,cb) +d(ca,cb) =z(b,a) =d(b,cb) +d(a,ca) +d(cb,ca) = [n\u2211 j=1(aj\u2212caj)2]1 2+ [n\u2211 j=1(bj\u2212cbj)2]1 2+ [n\u2211 j=1(caj\u2212cbj)2]1 2(13) z(a,b) =d(a,b) +\u00b5\u2217d(ca,cb) =z(b,a) =d(b,a) +\u00b5\u2217d(cb,ca) = [n\u2211 j=1(aj\u2212bj)2]1 2+\u00b5\u2217[n\u2211 j=1(caj\u2212cbj)2]1 2(14) From Eqs (13) and (14), we can see that the proposed Z-distance has symmetry. Proof of Property 3. For Property 3, i.e., the proposed Z- distance satis\ufb01es the directness in the following two cases. (1) When data points a,bandebelong to the same class. According to Eq (11), we can get the following formula: z(a,b) +z(b,e)\u2212z(a,e) =d(a,ca) +d(b,ca) +d(b,ca) +d(e,ca)\u2212d(a,ca)\u2212d(e,ca) = 2\u2217d(b,ca) = 2\u2217[n\u2211 j=1(bj\u2212caj)2]1 2\u22650(15)6 According to Eq (12) and trigonometric inequality, we can get the following formula: z(a,b) +z(b,e)\u2212z(a,e) =d(a,b) +d(b,e)\u2212d(a,e) = [n\u2211 j=1(aj\u2212bj)2]1 2+ [n\u2211 j=1(bj\u2212ej)2]1 2 +[n\u2211 j=1(aj\u2212ej)2]1 2\u22650(16) From Eqs (15) and (16), we can get that when a,bande belong to the same class, the proposed Z distance (Eqs (11) and (12)) have the property of directness. (2) When data points a,bandebelong to different classes. According to Eq (11) and trigonometric inequality, we can get the following formula: z(a,b) +z(b,e)\u2212z(a,e) =d(a,ca) +d(b,cb) +\u00b5\u2217d(ca,cb) +d(b,cb) +d(e,ce) +\u00b5\u2217d(cb,ce) \u2212d(a,ca)\u2212d(e,ce)\u2212\u00b5\u2217d(ca,ce) = 2\u2217d(b,cb) +\u00b5\u2217d(ca,cb) +\u00b5\u2217d(cb,ce)\u2212\u00b5\u2217d(ca,ce) = 2\u2217[n\u2211 j=1(bj\u2212cbj)2]1 2+\u00b5\u2217[n\u2211 j=1(caj\u2212cbj)2]1 2 +\u00b5\u2217[n\u2211 j=1(cbj\u2212cej)2]1 2+\u00b5\u2217[n\u2211 j=1(caj\u2212cej)2]1 2\u22650(17) According to Eq (12), we can get the following formula: z(a,b) +z(b,e)\u2212z(a,e) =d(a,b) +\u00b5\u2217d(ca,cb) +d(b,e) +\u00b5\u2217d(cb,ce) \u2212d(a,e)\u2212\u00b5\u2217d(ca,ce) = [n\u2211 j=1(aj\u2212bj)2]1 2+\u00b5\u2217[n\u2211 j=1(caj\u2212cbj)2]1 2 +[n\u2211 j=1(bj\u2212ej)2]1 2+\u00b5\u2217[n\u2211 j=1(cbj\u2212cej)2]1 2 \u2212[n\u2211 j=1(aj\u2212ej)2]1 2\u2212\u00b5\u2217[n\u2211 j=1(caj\u2212cej)2]1 2\u22650(18) According to Eqs (17) and (18), when a, b and e belong to different classes, the Z-distance (Eqs (11) and (12)) has the property of directness. In conclusion, the proposed Z-distance satis\ufb01es the property of directness. Corollary 1. Intraclass distance is less than interclass distance. Proof. For Eq (11), if data points a and b belong to the same class, then the Z distance between them is the following formula: z(a,b) =d(a,ca) +d(b,ca) (19) wherecais the class center of data points aandb. If data points aandebelong to different classes, the Z distance between them is the following formula: z(a,e) =d(a,ca) +d(e,ce) +\u00b5\u2217d(ca,ce) (20) From Eqs (19) and (20), we can see that as long as the following equations are proved to be true: d(e,ce) +\u00b5\u2217d(ca,ce)>d(b,ca) (21) Obviously, the distance between different classes is one more natural distance than that of the same class, i.e.,\u00b5\u2217d(ca,cb). If the value of parameter \u00b5is in\ufb01nite, then Eq (21) is sure to hold. When\u00b5takes a very small value, Eq (21) may not hold. Therefore,if the value of parameter \u00b5is large, then Eq (11) satis\ufb01es the characteristic that the distance between data of different class is always greater than distance between data of the same class. Similarly, for Eq (12), we only need to prove that the following formula holds: d(a,e) +\u00b5\u2217d(ca,ce)>d(a,b) (22) We can see that, as in Eq (11), if the value of parameter \u00b5is large, Eq (12) satis\ufb01es the characteristic that the distance between data of different class is always greater than distance between data of the same class. 3.3 Comparative analysis The Z distance is based on Euclidean distance, which can be regarded as an improvement of Euclidean distance. Compared with Euclidean distance, Z distance not only considers the natural distance, but also makes the distance between different classes greater than the distance between data in the same class. The properties and functions of Euclidean distance and Z distance are listed in Tables 1 and 2, respectively. The difference between our two distance metric functions and the traditional distance function is shown in Fig 4. The difference between them can be clearly seen from Fig 6, i.e.,Eq (11) achieves a larger class spacing, and Eq (12) not only achieves a larger class spacing, but also achieves a smaller distance in the data of same classi\ufb01cation. Euclideandistance Euclideandistance distance(Eq(12)) Zdistance(Eq(12)) distance(Eq(11)) Zdistance(Eq(11)) Fig. 6. The schematic diagram of three distance function comparisons 4 E XPERIMENTS In order to verify the validity of the new distance functions, we compare the KNN classi\ufb01cation accuracy of the new distance functions and the original KNN distance function ( i.e.,Euclidean distance function) with 12 data sets1 2(as shown in Table 3). 4.1 Experiment settings We download the data sets for our experiments from the datasets website, which includes 4 binary data sets and 8 multiclassi\ufb01cation data sets. We divide each data set into a training set and a test set by ten-fold cross-validation ( i.e.,we divide the data set into 10 parts, 9 of which are used as training sets, and the remaining one is used as a test set, which is sequentially cycled until all data 1. urlhttp://archive.ics.uci.edu/ml. 2. urlhttp://featureselection.asu.edu/datasets.php.7 TABLE 1 Properties of Euclidean distance and Z distance Distance function Nonnegativity Symmetry Directness Intraclass distance is less than interclass distance Euclidean distance \u2713 \u2713 \u2713 \u00d7 Z distance \u2713 \u2713 \u2713 \u2713 TABLE 2 Function comparison between Euclidean distance and Z distance Distance function Measuring af\ufb01nity Reachable distance Euclidean distance \u2713 \u00d7 Z distance \u2713 \u2713 TABLE 3 The information of the data sets DatasetsNumber of samplesDimensions Classes Banknote 1372 4 2 Cnae 1080 856 9 Drift 1244 129 5 Secom 1567 590 2 Ionosphere 351 34 2 Usps 9298 256 10 Yeast 1484 1470 10 Letter 20000 16 26 Movements 360 90 15 Multiple 2000 649 10 Statlog 6435 36 6 German 1000 20 2 have been tested). The algorithm is introduced as follows during the experiment: KNN [37]: It\u2019s the most traditional KNN algorithm, and we don\u2019t have to do anything during the training phase. In the test phase, for each test data point, we \ufb01nd its K neighbors in the training data according to the Euclidean distance. Then, the class label with the highest frequency of class in the K neighbors is selected as the \ufb01nal class label of the data. Until all test data is tested. NCP-KNN: This method is the most basic algorithm after introducing the class center point. In the training phase, a class center point is obtained for the training data in each class. In the test phase, we calculate the distance between each test data and the center point of the training process. The class of the nearest class center point is the class label of the test data point. CFKNN [38]: In this method, a new metric function is pro- posed, which is expressed linearly by test data. Speci\ufb01cally, it \ufb01rst uses training data to represent test data through least squares loss. Then it gets the relational metric matrix by solving the least squares loss. Finally, it uses the new metric matrix to construct a new distance function. It classi\ufb01es the test data according to the new distance function and major rule. LMRKNN [39]: It is an improved KNN method based on local mean vector representation. Speci\ufb01cally, it \ufb01rst \ufb01nds K neighbors in each class and constructs a local mean vector. Then it uses these local mean vectors to represent each test data and obtains a relationship measurement matrix. Finally, it uses the matrix to construct a new distance function for KNN.Z0-KNN: It is the traditional KNN method based on the Z0distance metric function ( i.e., Eq (11)). During the training process, we calculate the center point of each class in the training data, and calculate the center point of training data. In the test process, we \ufb01nd K neighbors from training data according to Eq (11). And then, we use the majority rule to predict the class label of test data. Z-KNN: It is the traditional KNN method based on the Z distance metric function ( i.e.,Eq (12)). It is basically the same asZ0-KNN\u2019s training process and testing process. The only difference is that it is based on Eq (12). For the above algorithms, we did a series of experiments. Speci\ufb01cally, for each data set, we test all the algorithms by setting different K values ( i.e.,1-10), where the NCP- KNN algorithm has no K parameter, so we have performed 10 experiments for it. It is convenient for us to put all the algorithms in one subgraph. Finally, we measure their performance based on classi\ufb01cation accuracy. In addition, in the case of K = 5, we performed 10 experiments on all algorithms to preserve the average classi\ufb01cation accuracy and standard deviation. Finally, for the binary classi\ufb01cation dataset, we not only calculated their classi\ufb01cation accuracy, but also calculated their Sensitivity (Sen) and Speci\ufb01city (Spe). The formulas for accuracy(Acc), and standard devia- tion(std)are as follows: Acc=Xcorrect /X (23) whereXcorrect represents the number of test data that is correctly classi\ufb01ed, and X represents the total number of test data. std=\u221a 1 nn\u2211 i=1(Acci\u2212\u00b5)2(24) wherenrepresents the number of experiments, Accirepresents the classi\ufb01cation accuracy of the i-th experiment, and \u00b5represents the average classi\ufb01cation accuracy of the experiment. The smaller the std, the more stable the algorithm is. 4.2 Binary classi\ufb01cation Table 4 shows the classi\ufb01cation effect of all algorithms on the binary dataset. We can get the some result, i.e.,the Z-KNN algo- rithm achieves the best results, and Ncp-KNN performs the worst. Speci\ufb01cally, on the German dataset, the classi\ufb01cation accuracy of the Z-KNN algorithm is 7.14% higher than the traditional KNN algorithm. The reason for this effect is that the distance function used by the Z-KNN algorithm not only makes the distance between similar data points close, but also makes the distance between data points of different class larger. It makes the data more separable, which makes the subsequent classi\ufb01cation better.8 1 2 3 4 5 6 7 8 9 10 K4045505560Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (a) Banknote 1 2 3 4 5 6 7 8 9 10 K2468101214Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (b) Cnae 1 2 3 4 5 6 7 8 9 10 K101520253035Acc(%) KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (c) Drift 1 2 3 4 5 6 7 8 9 10 K10203040506070Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (d) German 1 2 3 4 5 6 7 8 9 10 K10203040506070Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (e) Ionosphere 1 2 3 4 5 6 7 8 9 10 K11.522.533.544.55Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (f) Letter 1 2 3 4 5 6 7 8 9 10 K24681012Acc(%) KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (g) Movements 1 2 3 4 5 6 7 8 9 10 K24681012Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (h) Multiple 1 2 3 4 5 6 7 8 9 10 K2030405060708090100Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (i) Secom 1 2 3 4 5 6 7 8 9 10 K510152025Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (j) Stalog 1 2 3 4 5 6 7 8 9 10 K2468101214Acc(%)KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (k) Usps 1 2 3 4 5 6 7 8 9 10 K101520253035Acc(%) KNN NCP-KNN CFKNN LMRKNN Z0-KNN Z-KNN (l) Yeast Fig. 7. The classi\ufb01cation accuracy on the 12 datasets with different K values 4.3 Multiple classi\ufb01cation Figure 7 shows the classi\ufb01cation accuracy of all algorithms on 12 data sets as K value. Speci\ufb01cally, we can see that the effect of the Z-KNN algorithm is best in most cases from Figure 7. The NCP-KNN algorithm has the worst effect, and the overall effect of theZ0-KNN algorithm is not satisfactory, but it achieves the best effect on the Usps dataset, which shows that after we introduce the class feature information, it has a certain effect. The effect ofZ-KNN is suf\ufb01cient to prove that we are looking for a distance function with \u201chigh cohesion, low coupling\u201d is very necessary for classi\ufb01cation. For the traditional KNN algorithm, its effect is better than the NCP-KNN algorithm, which shows that only considering class information is unreliable. Table 5 shows the average classi\ufb01cation accuracy of the algorithm on the multi-class dataset. From Table 5, we can see that the Z-KNN algorithm achieves the best performance on the9  K 0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (a) Banknote  K  0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (b) Cnae   K0 120 2Acc(%) 340 4 10 5 98 6 7 7 65 8 4 9 32 101 (c) Drift   K0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (d) German   K0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (e) Ionosphere   K0 12Acc(%) 35 4 10 5 98 6 7 7 65 8 4 9 32 101 (f) Letter  K  0 15 2Acc(%) 310 4 10 5 98 6 7 7 65 8 4 9 32 101 (g) Movements   K0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (h) Multiple   K0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (i) Secom  K 0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (j) Stalog  K  0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (k) Usps  K  0 120 2Acc(%) 340 4 10 5 98 6 7 7 65 8 4 9 32 101 (l) Yeast Fig. 8. The classi\ufb01cation accuracy of different K and \u00b5parameter (in Eq(11)) values on the dataset. multi-class dataset except the Yeast dataset. The worst performer is the NCP-KNN algorithm. Table 6 shows the std of all algorithms in 10 experiments. From Table 6, we can see that the std of all algorithms is relatively small, that is, their stability is very good, and the difference is not big. Of course, there is also a special case. On the Letter dataset, the traditional KNN method has a variance of 9.86, which indicates that its stability on this dataset is not good. However, overall, the stability of all algorithms is verygood. 4.4 Parameter sensitivity In Eqs (11) and (12), there is a parameter \u00b5, which determines the size of the natural distance. Therefore, we set up experiments with different K values and different \u00b5values. As shown in Figs 8 and 9, we can see that in most cases, the value of \u00b5has an impact on the performance of KNN. Speci\ufb01cally, on the Drift,10   K0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (a) Banknote  K  0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (b) Cnae  K  0 120 2Acc(%) 340 4 10 5 98 6 7 7 65 8 4 9 32 101 (c) Drift   K0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (d) German  K  0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (e) Ionosphere  K  0 12Acc(%) 35 4 10 5 98 6 7 7 65 8 4 9 32 101 (f) Letter  K  0 15 2Acc(%) 310 4 10 5 98 6 7 7 65 8 4 9 32 101 (g) Movements  K 0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (h) Multiple   K0 150 2Acc(%) 3100 4 10 5 98 6 7 7 65 8 4 9 32 101 (i) Secom   K0 120 2Acc(%) 340 4 10 5 98 6 7 7 65 8 4 9 32 101 (j) Stalog  K  0 110 2Acc(%) 320 4 10 5 98 6 7 7 65 8 4 9 32 101 (k) Usps  K  0 120 2Acc(%) 340 4 10 5 98 6 7 7 65 8 4 9 32 101 (l) Yeast Fig. 9. The classi\ufb01cation accuracy of different K and \u00b5parameter (in Eq (12)) values on the dataset. Cnae, and Movements data sets, the accuracy rate varies greatly under different \u00b5values. This shows that one has to adjust the value of parameter \u00b5carefully. 5 C ONCLUSION This paper has proposed a new distance metric function, Z distance, by considering the characteristics of the class features. Speci\ufb01cally, we \ufb01rst considered the class center point into thedistance metric function, and then make the distance between data points of different class larger by calculating the distance between the class center point and the class center point, thus making the data more separable. Finally, the effect of high cohesion was achieved by directly calculating the Euclidean distance between data points in the same classi\ufb01cation. In the experiment, the proposed algorithm has achieved good results in both the binary data set and the multi-class data set.11 TABLE 4 Experimental results for binary data sets. Datasets Banknote German Ionosphere Secom ACC SEN SPE ACC SEN SPE ACC SEN SPE ACC SEN SPE KNN 54.59 49.02 59.06 61.56 80.29 18.50 58.20 77.78 23.97 92.26 0.03 98.77 NCP-KNN 52.15 47.87 57.35 56.29 67.29 31.00 52.91 56.53 46.43 64.86 56.35 42.95 CFKNN 52.53 48.12 53.45 61.23 77.64 22.93 56.92 66.23 34.64 90.36 4.04 95.95 LMRKNN 55.27 49.65 55.45 56.67 66.33 34.13 52.81 56.63 46.04 75.37 25.67 76.34 Z0-KNN 54.45 48.52 57.09 56.42 67.57 30.00 58.40 79.56 25.79 75.94 22.69 75.35 Z-KNN 56.56 51.15 60.89 68.70 93.00 0.06 60.68 79.87 21.43 92.98 0.01 99.25 TABLE 5 Average classi\ufb01cation accuracy on multi-class datasets (acc (%)) Datasets Cnae Drift Usps Yeast Letter Movements Multiple Statlog KNN 10.65 28.78 10.43 30.86 3.87 7.26 10.02 19.35 NCP-KNN 10.56 25.88 10.53 29.25 3.84 6.39 9.95 18.32 CFKNN 11.52 34.49 10.43 21.57 3.82 7.42 10.34 20.37 LMRKNN 12.18 26.37 11.61 28.43 3.86 6.97 10.78 17.13 Z0-KNN 11.76 24.28 11.52 31.67 3.86 6.94 10.50 18.77 Z-KNN 12.59 31.43 11.53 31.40 4.06 9.72 10.80 20.17 TABLE 6 Standard deviation of classi\ufb01cation accuracy Datasets Cnae Drift Usps Yeast Letter Movements Multiple Statlog KNN 0.01 0.02 0.00 0.00 9.86 0.02 0.01 0.00 NCP-KNN 0.01 0.01 0.00 0.01 0.00 0.01 0.01 0.00 CFKNN 0.01 0.01 0.01 0.01 0.00 0.01 0.01 0.02 LMRKNN 0.02 0.02 0.01 0.01 0.01 0.01 0.01 0.01 Z0-KNN 0.01 0.01 0.00 0.01 0.00 0.01 0.00 0.00 Z-KNN 0.01 0.01 0.00 0.00 0.00 0.02 0.00 0.00 The Z distance measurement function takes some information about the class features into account. Its core idea is to make the distance between data points of different class must be greater than the distance of data points in the same class, i.e.,\u201chigh coupling, low cohesion.\u201d In the future work, we plan to proceed from the following three points as follows. 1. Finding one or more better distance metric functions to make the K-nearest neighbor classi\ufb01cation algorithm achieve better performance. 2. Applying this idea to other classi\ufb01cation algorithms to \ufb01nd distance metric functions that are suitable for other classi\ufb01cation algorithms. 3. We will \ufb01nd a new distance function to apply to clustering, it is very challenging and interesting. ACKNOWLEDGMENT This work is partially supported by the Key Program of the Na- tional Natural Science Foundation of China (Grant No: 61836016).REFERENCES [1] N. Kumar and K. Kummamuru, \u201cSemisupervised clustering with metric learning using relative comparisons,\u201d IEEE Transactions on Knowledge and Data Engineering , vol. 20, no. 4, pp. 496\u2013503, 2008. [2] X. Zhu, S. Zhang, Y . Zhu, P. Zhu, and Y . Gao, \u201cUnsupervised spectral feature selection with dynamic hyper-graph learning,\u201d IEEE Transactions on Knowledge and Data Engineering , 2020. [3] X. Zhu, S. Zhang, Y . Li, J. Zhang, L. Yang, and Y . Fang, \u201cLow- rank sparse subspace for spectral clustering,\u201d IEEE Transactions on Knowledge and Data Engineering , vol. 31, no. 8, pp. 1532\u20131543, 2018. [4] Y . Guo, Z. Cheng, J. Jing, Y . Lin, L. Nie, and M. Wang, \u201cEnhancing factorization machines with generalized metric learning,\u201d IEEE Transac- tions on Knowledge and Data Engineering , 2020. [5] Z. Tang, L. Chen, X. Zhang, and S. Zhang, \u201cRobust image hashing with tensor decomposition,\u201d IEEE Transactions on Knowledge and Data Engineering , vol. 31, no. 3, pp. 549\u2013560, 2018. [6] C. Zhu, L. Cao, Q. Liu, J. Yin, and V . Kumar, \u201cHeterogeneous metric learning of categorical data with hierarchical couplings,\u201d IEEE Transac- tions on Knowledge and Data Engineering , vol. 30, no. 7, pp. 1254\u20131267, 2018. [7] X.-S. Wei, H.-J. Ye, X. Mu, J. Wu, C. Shen, and Z.-H. Zhou, \u201cMultiple instance learning with emerging novel class,\u201d IEEE Transactions on Knowledge and Data Engineering , 2019. [8] X. Zhu, S. Zhang, W. He, R. Hu, C. Lei, and P. Zhu, \u201cOne-step multi- view spectral clustering,\u201d IEEE Transactions on Knowledge and Data Engineering , vol. 31, no. 10, pp. 2022\u20132034, 2018.12 [9] S. P. Patel and S. Upadhyay, \u201cEuclidean distance based feature ranking and subset selection for bearing fault diagnosis,\u201d Expert Systems with Applications , vol. 154, p. 113400, 2020. [10] X. Gao and G. Li, \u201cA knn model based on manhattan distance to identify the snare proteins,\u201d IEEE Access , vol. 8, pp. 112 922\u2013112 931, 2020. [11] I. B. K. D. S. Negara and I. P. P. Wardana, \u201cIdenti\ufb01kasi kecocokan motif tenun songket khas jembrana dengan metode manhattan distance,\u201d Jurnal Teknologi Informasi dan Komputer , vol. 7, no. 2, 2021. [12] K. Chomboon, P. Chujai, P. Teerarassamee, K. Kerdprasop, and N. Kerd- prasop, \u201cAn empirical study of distance metrics for k-nearest neighbor algorithm,\u201d in Proceedings of the 3rd international conference on indus- trial application engineering , 2015, pp. 280\u2013285. [13] H. Ji, \u201cStatistics mahalanobis distance for incipient sensor fault detection and diagnosis,\u201d Chemical Engineering Science , vol. 230, p. 116233, 2021. [14] L. Aggoun and Y . Chetouani, \u201cFault detection strategy combining narmax model and bhattacharyya distance for process monitoring,\u201d Journal of the Franklin Institute , vol. 358, no. 3, pp. 2212\u20132228, 2021. [15] S. Ji, Z. Zhang, S. Ying, L. Wang, X. Zhao, and Y . Gao, \u201cKullback-leibler divergence metric learning,\u201d IEEE Transactions on Cybernetics , 2020. [16] Q. Zhang, X. Guan, H. Wang, and P. M. Pardalos, \u201cMaximum shortest path interdiction problem by upgrading edges on trees under hamming distance,\u201d Optimization Letters , pp. 1\u201320, 2021. [17] P. Ganesan, B. Sathish, L. L. Joseph, K. Subramanian, and R. Murugesan, \u201cThe impact of distance measures in k-means clustering algorithm for natural color images,\u201d in Advances in Arti\ufb01cial Intelligence and Data Engineering . Springer, 2021, pp. 947\u2013963. [18] M. S. Santos, P. H. Abreu, S. Wilk, and J. Santos, \u201cHow distance metrics in\ufb02uence missing data imputation with k-nearest neighbours,\u201d Pattern Recognition Letters , vol. 136, pp. 111\u2013119, 2020. [19] J. Gou, H. Ma, W. Ou, S. Zeng, Y . Rao, and H. Yang, \u201cA generalized mean distance-based k-nearest neighbor classi\ufb01er,\u201d Expert Systems with Applications , vol. 115, pp. 356\u2013372, 2019. [20] Z. Geler, V . Kurbalija, M. Ivanovi \u00b4c, and M. Radovanovi \u00b4c, \u201cWeighted knn and constrained elastic distances for time-series classi\ufb01cation,\u201d Expert Systems with Applications , vol. 162, p. 113829, 2020. [21] P. A. Poorheravi, B. Ghojogh, V . Gaudet, F. Karray, and M. Crowley, \u201cAcceleration of large margin metric learning for nearest neighbor classi\ufb01cation using triplet mining and strati\ufb01ed sampling,\u201d arXiv preprint arXiv:2009.14244 , 2020. [22] M. Feng, M. Li, and S. Xu, \u201cProject 2: Knn with different distance metrics.\u201d [23] K. Song, F. Nie, J. Han, and X. Li, \u201cParameter free large margin nearest neighbor for distance metric learning,\u201d in Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , vol. 31, no. 1, 2017. [24] Y .-K. Noh, B.-T. Zhang, and D. D. Lee, \u201cGenerative local metric learn- ing for nearest neighbor classi\ufb01cation,\u201d IEEE transactions on pattern analysis and machine intelligence , vol. 40, no. 1, pp. 106\u2013118, 2017. [25] S. Ying, Z. Wen, J. Shi, Y . Peng, J. Peng, and H. Qiao, \u201cManifold preserving: An intrinsic approach for semisupervised distance metric learning,\u201d IEEE transactions on neural networks and learning systems , vol. 29, no. 7, pp. 2731\u20132742, 2017. [26] D. Wang and X. Tan, \u201cRobust distance metric learning via bayesian inference,\u201d IEEE Transactions on Image Processing , vol. 27, no. 3, pp. 1542\u20131553, 2017. [27] L. Jiao, X. Geng, and Q. Pan, \u201cBp knn:k-nearest neighbor classi\ufb01er with pairwise distance metrics and belief function theory,\u201d IEEE Access , vol. 7, pp. 48 935\u201348 947, 2019. [28] Y . Song, Y . Gu, R. Zhang, and G. Yu, \u201cBrepartition: Optimized high- dimensional knn search with bregman distances,\u201d IEEE Transactions on Knowledge and Data Engineering , 2020. [29] B. Su and Y . Wu, \u201cLearning meta-distance for sequences by learning a ground metric via virtual sequence regression,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020. [30] N. Marchang and R. Tripathi, \u201cKnn-st: Exploiting spatio-temporal corre- lation for missing data inference in environmental crowd sensing,\u201d IEEE Sensors Journal , vol. 21, no. 3, pp. 3429\u20133436, 2020. [31] L. A. C. Valverde and J. A. M. Arias, \u201cEvaluaci \u00b4on de distintas t \u00b4ecnicas de representaci \u00b4on de texto y medidas de distancia de texto usando knn para clasi\ufb01caci \u00b4on de documentos,\u201d Tecnolog \u00b4\u0131a en Marcha , vol. 33, no. 1, pp. 64\u201379, 2020. [32] \u00a8O. F. Ertu \u02d8grul, \u201cA novel distance metric based on differential evolution,\u201d Arabian Journal for Science and Engineering , vol. 44, no. 11, pp. 9641\u2013 9651, 2019. [33] Y .-P. Sun and M.-L. Zhang, \u201cCompositional metric learning for multi- label classi\ufb01cation,\u201d Frontiers of Computer Science , vol. 15, no. 5, pp. 1\u201312, 2021.[34] X. Gu, P. P. Angelov, D. Kangin, and J. C. Principe, \u201cA new type of distance metric and its use for clustering,\u201d Evolving Systems , vol. 8, no. 3, pp. 167\u2013177, 2017. [35] L. Gong, H. Wang, M. Ogihara, and J. Xu, \u201cidec: indexable distance estimating codes for approximate nearest neighbor search,\u201d Proceedings of the VLDB Endowment , vol. 13, no. 9, pp. 1483\u20131497, 2020. [36] S. Susan and A. Kumar, \u201cDst-ml-eknn: data space transformation with metric learning and elite k-nearest neighbor cluster formation for clas- si\ufb01cation of imbalanced datasets,\u201d in Advances in Arti\ufb01cial Intelligence and Data Engineering . Springer, 2021, pp. 319\u2013328. [37] U. Lall and A. Sharma, \u201cA nearest neighbor bootstrap for resampling hydrologic time series,\u201d Water Resources Research , vol. 32, no. 3, pp. 679\u2013693, 1996. [38] Y . Xu, Q. Zhu, Z. Fan, M. Qiu, Y . Chen, and H. Liu, \u201cCoarse to \ufb01ne k nearest neighbor classi\ufb01er,\u201d Pattern recognition letters , vol. 34, no. 9, pp. 980\u2013986, 2013. [39] J. Gou, W. Qiu, Z. Yi, Y . Xu, Q. Mao, and Y . Zhan, \u201cA local mean representation-based k-nearest neighbor classi\ufb01er,\u201d ACM Transactions on Intelligent Systems and Technology (TIST) , vol. 10, no. 3, pp. 1\u201325, 2019. Shichao Zhang is a China National Distin- guished Professor with the Central South Uni- versity, China. He holds a PhD degree from the Deakin University, Australia. His research interests include data mining and big data. He has published 90 international journal papers and over 70 international conference papers. He is a CI for 18 competitive national grants. He serves/served as an associate editor for four journals. Jiaye Li is a PhD student at Central South Uni- versity. His research interests include machine learning, data mining and deep learning.",
        "Interestingness": -1,
        "Feasibility": -1,
        "Novelty": -1,
        "novel": true
    },
    {
        "Name": "proto_nn",
        "Title": "ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices",
        "Experiment": "Below is the full text of the paper with given title. Your goal is to change the code so it reproduces the idea. There are three levels of reproducibility that you might be able to check for: 1) Replicate everything 1:1 and just check if the obtained results are the same as in the paper; 2) Do just a slight variation, e.g. a different random seed, different datasets, etc. to verify that the results are not cherry picked; 3) Do larger variation, e.g. changes in the algorithm itself. Here comes the original paper:\nProtoNN: Compressed and Accurate kNN for Resource-scarce Devices Chirag Gupta1Arun Sai Suggala1 2Ankit Goyal1 3Harsha Vardhan Simhadri1 Bhargavi Paranjape1Ashish Kumar1Saurabh Goyal4Raghavendra Udupa1Manik Varma1 Prateek Jain1 Abstract Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applica- tions demand prediction models with small stor- age and computational complexity that do not compromise signi\ufb01cantly on accuracy. In this work, we propose ProtoNN, a novel algorithm that addresses the problem of real-time and accu- rate prediction on resource-scarce devices. Pro- toNN is inspired by k-Nearest Neighbor (KNN) but has several orders lower storage and predic- tion complexity. ProtoNN models can be de- ployed even on devices with puny storage and computational power (e.g. an Arduino UNO with 2kB RAM) to get excellent prediction accu- racy. ProtoNN derives its strength from three key ideas: a) learning a small number of prototypes to represent the entire training set, b) sparse low dimensional projection of data, c) joint discrim- inative learning of the projection and prototypes with explicit model size constraint. We conduct systematic empirical evaluation of ProtoNN on a variety of supervised learning tasks (binary, multi-class, multi-label classi\ufb01cation) and show that it gives nearly state-of-the-art prediction ac- curacy on resource-scarce devices while consum- ing several orders lower storage, and using mini- mal working memory. 1. Introduction Real-time and accurate prediction on resource-constrained devices is critical for several Machine Learning (ML) do- 1Microsoft Research, India2Carnegie Mellon Uni- versity, Pittsburgh3University of Michigan, Ann Arbor 4IIT Delhi, India. Correspondence to: Arun Sai Sug- gala <asuggala@andrew.cmu.edu >, Prateek Jain <pra- jain@microsoft.com >. Proceedings of the 34thInternational Conference on Machine Learning , Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).mains. Internet-of-things (IoT) is one such rapidly grow- ing domain. IoT devices have the potential to provide real- time, local, sensor-based solutions for a variety of areas like housing, factories, farming, even everyday utilities like toothbrushes and spoons. The ability to use machine learn- ing on data collected from IoT sensors opens up a myriad of possibilities. For example, smart factories measure tem- perature, noise and various other parameters of their ma- chines. ML based anomaly detection models can then be applied on this sensor data to preemptively schedule main- tenance of a machine and avoid failure. However, machine learning in IoT scenarios is so far lim- ited to cloud-based predictions where large deep learning models are deployed to provide accurate predictions. The sensors/embedded devices have limited compute/storage abilities and are tasked only with sensing and transmitting data to the cloud. Such a solution does not take into account several practical concerns like privacy, bandwidth, latency and battery issues. For example, consider the energy costs of communication if each IoT device on each machine in a smart factory has to continuously send data and receive predictions from the cloud. Consider a typical IoT device that has \u226432kB RAM and a16MHz processor. Most existing ML models cannot be deployed on such tiny devices. Recently, several methods (Han et al., 2016; Nan et al., 2015; Kusner et al., 2014) have been proposed to produce models that are compressed com- pared to large DNN/kernel-SVM/decision-tree based clas- si\ufb01ers. However, none of these methods work well at the scale of IoT devices. Moreover, they do not offer natural extensions to supervised learning problems other than the ones they were initially designed for. In this paper, we propose a novel kNN based algorithm (ProtoNN) that can be deployed on the tiniest of devices, can handle general supervised learning problems, and can produce state-of-the-art accuracies with just \u224816kB of model size on many benchmark datasets. A key reason for selecting kNN as the algorithm of choice is due to its gen- erality, ease of implementation on tiny devices, and small number of parameters to avoid over\ufb01tting. However, kNN suffers from three issues which limit its applicability inProtoNN: kNN for Resource-scarce Devices practice, especially in the small devices setting: a) Poor accuracy : kNN is an ill-speci\ufb01ed algorithm as it is not a priori clear which distance metric one should use to com- pare a given set of points. Standard metrics like Euclidean distance,\u21131distance etc. are not task-speci\ufb01c and lead to poor accuracies. b) Model size : kNN requires the entire training data for prediction, so its model size is too large for the IoT setting. c) Prediction time : kNN requires com- puting the distance of a given test point w.r.t. each training point, making it prohibitive for prediction in real-time. Several methods have been proposed to address some of these concerns. For example, metric learning (Weinberger & Saul, 2009) learns a task-speci\ufb01c metric that provides better accuracies but ends up increasing model-size and prediction time. KD-trees (Bentley, 1975) can decrease the prediction time, but they increase the model size and lead to loss in accuracy. Finally, recent methods like Stochastic Neighborhood Compression (SNC) (Kusner et al., 2014) can decrease model size and prediction time by learning a small number of prototypes to represent the entire training dataset. However, as our experiments show, their predic- tions are relatively inaccurate, especially in the tiny model- size regime. Moreover, their formulations limit applicabil- ity to binary and multi-class classi\ufb01cation problems (see Section 2 for a detailed comparison to SNC). In contrast, ProtoNN is able to address the above- mentioned concerns by using three key ideas: a)Sparse low-d projection : we project the entire data in low-d using a sparse projection matrix that is jointly learned to provide good accuracy in the projected space. b)Prototypes : we learn prototypes to represent the entire training dataset. Moreover, we learn labels for each pro- totype to further boost accuracy. This provides additional \ufb02exibility , and allows us to seamlessly generalize ProtoNN for multi-label or ranking problems. c)Joint optimization : we learn the projection matrix jointly with the prototypes and their labels. Explicit sparsity con- straints are imposed on our parameters during the optimiza- tion itself so that we can obtain an optimal model within the given model size de-facto, instead of post-facto pruning to force the model to \ufb01t in memory. Unfortunately, our optimization problem is non-convex with hard\u21130constraints. Yet, we show that simple stochas- tic gradient descent (SGD) with iterative hard-thresholding (IHT) works well for optimization. ProtoNN can be im- plemented ef\ufb01ciently, can handle datasets with millions of points, and obtains state-of-the-art accuracies. We analyze ProtoNN in a simple binary classi\ufb01cation set- ting where the data is sampled from a mixture of two well- separated Gaussians, each Gaussian representing one class.We show that if we \ufb01x the projection matrix and proto- type labels, the prototypes themselves can be learned op- timally in polynomial time with at least a constant proba- bility. Moreover, assuming a strong initialization condition we observe that our SGD+IHT method when supplied a small number of samples, proportional to the sparsity of means, converges to the global optima. Although the data model is simple, it nicely captures the main idea behind our problem formulation. Further, our analysis is the \ufb01rst such analysis for any method in this regime that tries to learn a compressed non-linear model for binary classi\ufb01cation. Finally, we conduct extensive experiments to benchmark ProtoNN against existing state-of-the-art methods for var- ious learning tasks. First, we show that on several bi- nary (multi-class) problems, ProtoNN with a 2kB (16kB) memory budget signi\ufb01cantly outperforms all the existing methods in this regime. Moreover, in the binary classi- \ufb01cation case, we show that ProtoNN with just \u224816kB of model-size, provides nearly the same accuracy as most popular methods like GBDT, RBF-SVM, 1-hidden layer NN, etc, which might require up to 1GB of RAM on the same datasets. Similarly, on multilabel datasets, ProtoNN can give 100\u00d7compression with \u22641%loss in accuracy. Finally, we demonstrate that ProtoNN can be deployed on a tiny Arduino Uno device1and leads to better accuracies than existing methods while incurring signi\ufb01cantly lesser energy and prediction time costs. We have implemented ProtoNN as part of an open source embedded device ML library and it can be downloaded online2. 2. Related Works kNN is a popular ML algorithm owing to its simplicity, generality, and interpretability (Cover & Hart, 2006). In particular, kNN can learn complex decision boundaries and has only one hyperparameter k. However, vanilla kNN suf- fers from several issues as mentioned in the previous sec- tion. A number of methods, which try to address these is- sues, exist in the literature. Broadly, these methods can be divided into three sub-categories. Several existing methods reduce prediction time of kNN using fast nearest neighbor retrieval. For example Bent- ley (1975); Beygelzimer et al. (2006) use tree data struc- tures and Gionis et al. (1999); Weiss et al. (2008); Kulis & Darrell (2009); Norouzi et al. (2012); Liu et al. (2012) learn binary embeddings for fast nearest neighbor opera- tions. These methods, although helpful in reducing the pre- diction time, lead to loss in accuracy and require the entire training data to be in memory leading to large model sizes that cannot be deployed on tiny IoT devices. 1https://www.arduino.cc/en/Main/ArduinoBoardUno 2https://github.com/Microsoft/ELLProtoNN: kNN for Resource-scarce Devices Another class of methods improve accuracy of kNN by learning a better metric to compare, given a pair of points (Goldberger et al., 2004; Davis et al., 2007). For example, (Weinberger & Saul, 2009) proposed a Large Margin Near- est Neighbor (LMNN) classi\ufb01er which transforms the input space such that in the transformed space points from same class are closer compared to points from disparate classes. LMNN\u2019s transformation matrix can map data into lower di- mensions and reduce overall model size compared to kNN, but it is still too large for most resource-scarce devices. Finally, another class of methods constructs a set of pro- totypes to represent the entire training data. In some ap- proaches (Angiulli, 2005; Devi & Murty, 2002), the pro- totypes are chosen from the original training data, while some other approaches (Mollineda et al., 2002) construct arti\ufb01cial points for prototypes. Of these approaches, SNC, Deep SNC (DSNC) (Wang et al., 2016), Binary Neighbor Compression (BNC) (Zhong et al., 2017) are the current state-of-the-art. SNC learns arti\ufb01cial prototypes such that the likelihood of a particular class probability model is maximized. Thus, SNC applies only to multi-class problems and its extension to multilabel/ranking problems is non-trivial. In contrast, we have a more direct discriminative formulation that can be applied to arbitrary supervised learning problems. To decrease the model size, SNC introduces a pre-processing step of low-d projection of the data via LMNN based pro- jection matrix and then learns prototypes in the projected space. The SNC parameters (projection matrix, prototypes) might have to be hard-thresholded post-facto to \ufb01t within the memory budget. In contrast, ProtoNN\u2019s parameters arede-facto learnt jointly with model size constraints im- posed during optimization. This leads to signi\ufb01cant im- provements over SNC and other state-of-the-art methods in the small model-size regime; see Figure 1, 3. DSNC is a non-linear extension of SNC in that it learns a non-linear low-d transformation jointly with the proto- types. It has similar drawbacks as SNC: a) it only applies to multi-class problems and b) model size of DSNC can be signi\ufb01cantly larger than SNC as it uses a feedforward network to learn the non-linear transformation. BNC is a binary embedding technique, which jointly learns a binary embedding and a set of arti\ufb01cial binary prototypes. Although BNC learns binary embeddings, its dimensional- ity can be signi\ufb01cantly higher, so it need not result in sig- ni\ufb01cant model compression. Moreover, the optimization in BNC is dif\ufb01cult because of the discrete optimization space. 3. Problem Formulation Givenndata pointsX= [x1,x2,...xn]Tand the corre- sponding target output Y= [y1,y2...yn]T, where xi\u2208Rd,yi\u2208Y, our goal is to learn a model that accurately pre- dicts the desired output of a given test point. In addition, we also want our model to have small size. For both multi- label/multi-class problems with Llabels, yi\u2208{0,1}Lbut in multi-class\u2225yi\u2225= 1. Similarly, for ranking problems, the outputyiis a permutation. Let\u2019s consider a smooth version of kNN prediction function for the above given general supervised learning problem \u02c6y=\u03c1(\u02c6s) =\u03c1(n\u2211 i=1\u03c3(yi)K(x,xi)) , (1) where \u02c6yis the predicted output for a given input x,\u02c6s=\u2211n i=1\u03c3(yi)K(x,xi)is the score vector for x.\u03c3:Y \u2192 RLmaps a given output into a score vector and \u03c1:RL\u2192 Ymaps the score function back to the output space. For example, in the multi-class classi\ufb01cation, \u03c3is the identity function while \u03c1=Top1, where [Top1(s)]j= 1 ifsjis the largest element and 0otherwise.K:Rd\u00d7Rd\u2192R is the similarity function, i.e., K(xi,xj)computes similar- ity between xiandxj. For example, standard kNN uses K(x,xi) =I[xi\u2208Nk(x)]whereNk(x)is the set of k nearest neighbors of xinX. Note that kNN requires entire Xto be stored in mem- ory for prediction, so its model size and prediction time are prohibitive for resource constrained devices. So, to bring down model and prediction complexity of kNN, we propose using prototypes that represent the entire train- ing data. That is, we learn prototypes B= [b1,...,bm] and the corresponding score vectors Z= [z1,...,zm]\u2208 RL\u00d7m, so that the decision function is given by: \u02c6y= \u03c1(\u2211m j=1zjK(x,bj)) . Existing prototype based approaches like SNC, DSNC have a speci\ufb01c probabilistic model for multi-class problems with the prototypes as the model parameters. In contrast, we take a more direct discriminative learning approach that al- lows us to obtain better accuracies in several settings along with generalization to any supervised learning problem, e.g., multi-label classi\ufb01cation, regression, ranking, etc. However,Kis a \ufb01xed similarity function like RBF kernel which is not tuned for the task at hand and can lead to in- accurate results. We propose to solve this issue by learning a low-dimensional matrix W\u2208R\u02c6d\u00d7dthat further brings down model/prediction complexity as well as transforms data into a space where prediction is more accurate.That is, our proposed algorithm ProtoNN uses the following predic- tion function that is based on three sets of learned param- etersW\u2208R\u02c6d\u00d7d,B= [b1,...,bm]\u2208R\u02c6d\u00d7m, andZ= [z1,...,zm]\u2208RL\u00d7m:\u02c6y=\u03c1(\u2211m j=1zjK(Wx,bj)) . To further reduce the model/prediction complexity, we learn sparse set of Z,B,W . Selecting the correct simi-ProtoNN: kNN for Resource-scarce Devices larity function Kis crucial to the performance of the algo- rithm. In this work we choose Kto be the Gaussian ker- nel:K\u03b3(x,y) = exp{\u2212\u03b32\u2225x\u2212y\u22252 2}, which is a popular choice in many non-parametric methods (including regres- sion, classi\ufb01cation, density estimation). Note that if m=n, andW=Id\u00d7d, then our prediction function reduces to the standard RBF kernel-SVM\u2019s deci- sion function for binary classi\ufb01cation. That is, our func- tion class is universal: we can learn any arbitrary function given enough data and model complexity. We observe a similar trend in our experiments, where even with reason- ably small amount of model complexity, ProotNN nearly matches RBF-SVM\u2019s prediction error. Training Objective : We now provide the formal optimiza- tion problem to learn parameters Z,B,W . LetL(\u02c6s,y)be the loss (or) risk of predicting score vector \u02c6sfor a point with label vector y. For example, the loss function can be standard hinge-loss for binary classi\ufb01cation, or NDCG loss function for ranking problems. Now, de\ufb01ne the empirical risk associated with Z,B,W as Remp(Z,B,W ) =1 nn\u2211 i=1L\uf8eb \uf8edyi,m\u2211 j=1zjK\u03b3(bj,Wxi)\uf8f6 \uf8f8. In the sequel, to simplify the notation, we denote the risk atithdata point byLi(Z,B,W )i.e.,Li(Z,B,W ) = L( yi,\u2211m j=1zjK\u03b3(bj,Wxi)) . To jointly learn Z,B,W , we minimize the empirical risk with explicit sparsity con- straints: min Z:\u2225Z\u22250\u2264sZ,B:\u2225B\u22250\u2264sB,W:\u2225W\u22250\u2264sWRemp(Z,B,W ),(2) where\u2225Z\u22250is equal to the number of non-zero entries in Z. For all our expeirments (multi-class/multi-label), we used the squared \u21132loss function as it helps us write down the gradients easily and allows our algorithm to converge faster and in a robust manner. That is, Remp(Z,B,W ) = 1 n\u2211n i=1\u2225yi\u2212\u2211m j=1zjK\u03b3(bj,Wxi)\u22252 2. Note that the sparsity constraints in the above objective gives us explicit control over the model size. Furthermore, as we show in our experiments, jointly optimizing all the three parame- ters,Z,B,W , leads to better accuracies than optimizing only a subset of parameters. 4. Algorithm We now present our algorithm for optimization of (2). Note that the objective in (2) is non-convex and is dif\ufb01cult to optimize. However, we present a simple alternating mini- mization technique for its optimization. In this technique, we alternately minimize Z,B,W while \ufb01xing the other two parameters. Note that the resulting optimization problemAlgorithm 1 ProtoNN: Train Algorithm Input: data(X,Y ), sparsities (sZ,sB,sW), kernel pa- rameter\u03b3, projection dimension \u02c6d, no. of prototypes m, iterationsT, SGD epochs e. InitializeZ,B,W fort= 1toTdo{alternating minimization } repeat{minimization of Z} randomly sample S\u2286[1,...n ] Z\u2190HTsZ( Z\u2212\u03b7r\u2211 i\u2208S\u2207ZLi(Z,B,W )) untileepochs repeat{minimization of B} randomly sample S\u2286[1,...n ] B\u2190HTsB( B\u2212\u03b7r\u2211 i\u2208S\u2207BLi(Z,B,W )) untileepochs repeat{minimization of W} randomly sample S\u2286[1,...n ] W\u2190HTsW( W\u2212\u03b7r\u2211 i\u2208S\u2207WLi(Z,B,W )) untileepochs end for Output:Z,B,W in each of the alternating steps is still non-convex. To opti- mize these sub-problems we use projected Stochastic Gra- dient Descent (SGD) for large datasets and projected Gra- dient Descent (GD) for small datasets. Suppose we want to minimize the objective w.r.t Zby \ufb01xingB,W . Then in each iteration of SGD we ran- domly sample a mini-batch S\u2286[1,...n ]and updateZ as:Z\u2190HTsZ( Z\u2212\u03b7\u2211 i\u2208S\u2207ZLi(Z,B,W )) , where HTsZ(A)is the hard thresholding operator that thresholds the smallest L\u00d7m\u2212sZentries (in magnitude) of Aand \u2207ZLi(Z,B,W )denotes the partial derivative of Liw.r.t Z. Note that GD procedure is just SGD with batch size |S|=n. Algorithm 1 presents pseudo-code for our entire training procedure. Step-size : Setting correct step-size is critical to conver- gence of SGD methods, especially for non-convex opti- mization problems. For our algorithm, we select the ini- tial step size using Armijo rule. Subsequent step sizes are selected as\u03b7t=\u03b70/twhere\u03b70is the initial step-size. Initialization: Since our objective function (2) is non- convex, good initialization for Z,B,W is critical in con- verging ef\ufb01ciently to a good local optima. We used a randomly sampled Gaussian matrix to initialize Wfor bi- nary and small multi-class benchmarks. However, for large multi class datasets ( aloi) we use LMNN based initializa- tion of W. Similarly, for multi-label datasets we use SLEEC (Bhatia et al., 2015) for initialization of W; SLEEC is an embedding technique for large multi-label problems. For initialization of prototypes, we experimented with two different approaches. In one, we randomly sample trainingProtoNN: kNN for Resource-scarce Devices data points in the transformed space and assign them as the prototypes; this is a useful technique for multilabel prob- lems. In the other approach, we run k-means clustering in the transformed space on data points belonging to each class and pick the cluster centers as our prototypes. We use this approach for binary and multi-class problems. Convergence : Although Algorithm 1 optimizes an \u21130con- strained optimization problem, we can still show that it converges to a local minimum due to smoothness of objec- tive function (Blumensath & Davies, 2008). Moreover, if the objective function satis\ufb01es strong convexity in a small ball around optima, then appropriate initialization leads to convergence to that optima (Jain et al., 2014). In fact, our next section presents such a strong convexity result (wrt B) if the data is generated from a mixture of well-separated Gaussians. Finally, our empirical results (Section 6) indi- cate that the objective function indeed converges at a fast rate to a good local optimum leading to accurate models. 5. Analysis In this section, we present an analysis of our approach for when data is generated from the following generative model: let each point xibe sampled from a mixture of two Gaussians, i.e., xii.i.d\u223c0.5\u00b7N(\u00b5+,I)+0.5\u00b7N(\u00b5\u2212,I)\u2208Rd and the corresponding label yibe the indicator of the Gaus- sian from which xiis sampled. Now, it is easy to see that if the Gaussians are well-separated then one can design 2 prototypes b+\u2217,b\u2212\u2217such that the error of our method with W=Iand \ufb01xedZ= [e1,e2]will lead to nearly Bayes\u2019 optimal classi\ufb01er; eiis thei-th canonical basis vector. The goal of this section is to show that our method that optimizes the squared \u21132loss objective (2) w.r.t. prototypes B, converges at a linear rate to a solution that is in a small ball around the global optima, and hence leads to nearly optimal classi\ufb01cation accuracy. We would like to stress that the goal of our analysis is to justify our proposed approach in a simple and easy to study setting. We do not claim new bounds for the mixture of Gaussians problem; it is a well-studied problem with sev- eral solid solutions. Our goal is to show that our method in this simple setting indeed converges to a nearly optimal solution at linear rate, thus providing some intuition for its success in practice. Also, our current analysis only studies optimization w.r.t. the prototypes Bwhile \ufb01xing projec- tion matrixWand prototype label vectors Z. Studying the problem w.r.t. all the three parameters is signi\ufb01cantly more challenging, and is beyond the scope of this paper. Despite the simplicity of our model, ours is one of the \ufb01rst rigorous studies of a classi\ufb01cation method that is designed for resource constrained problems. Typically, the proposed methods in this regime are only validated using empiricalresults as theoretical study is quite challenging owing to the obtained non-convex optimization surface and complicated modeling assumptions. For our \ufb01rst result, we ignore sparsity of B, i.e.,sB= 2\u00b7d. We consider the RBF-kernel for Kwith\u03b32=1 2. Theorem 1. LetX= [x1,...,xn]andY= [y1,...,yn] be generated from the above mentioned generative model. SetW=I,Z= [e1,e2]and let b+,b\u2212be the prototypes. Letn\u2192\u221e ,\u00af\u00b5:=\u00b5+\u2212\u00b5\u2212. Also, let \u2206+:=b+\u2212\u00b5+, \u2206\u2212:=b+\u2212\u00b5\u2212, and let \u2206+T\u00af\u00b5\u2265\u2212(1\u2212\u03b4) 2\u2225\u00af\u00b5\u22252for some \ufb01xed constant \u03b4 > 0, andd\u22658(\u03b1\u2212\u03b4)\u2225\u00af\u00b5\u22252for some constant\u03b1>0. Then, the following holds for the gradient descent step b+\u2032=b+\u2212\u03b7\u2207b+RwhereR=E[Remp], and\u03b7\u22650is appropriately chosen: \u2225b+\u2032\u2212\u00b5+\u22252\u2264\u2225b+\u2212\u00b5+\u22252( 1\u22120.01 exp{ \u2212\u03b1\u2225\u00af\u00b5\u22252 4}) , if\u2225\u2206+\u2225\u22658\u2225\u00af\u00b5\u2225exp{ \u2212\u03b1\u2225\u00af\u00b5\u22252 4} . See Appendix 8 for a detailed proof of this as well as the below given theorem. The above theorem shows that if the Gaussians are well-separated and the starting b+is closer to\u00b5+than\u00b5\u2212, then the gradient descent step decreases the distance between b+and\u00b5+geometrically until b+con- verges to a small ball around \u00b5+, the radius of the ball is exponentially small in \u2225\u00b5+\u2212\u00b5\u2212\u2225. Note that our initializa- tion method indeed satis\ufb01es the above mentioned assump- tion with at least a constant probability. It is easy to see that in this setting, the loss function decom- poses over independent terms from b+andb\u2212, and hence an identical result can be obtained for b\u2212. For simplicity, we present the result for n\u2192\u221e (hence, expected value). Extension to \ufb01nite samples should be fairly straightforward using standard tail bounds. The tail bounds will also lead to a similar result for SGD but with an added variance term. Next, we show that if the b+is even closer to \u00b5+, then the objective function becomes strongly convex inb+,b\u2212. Theorem 2. LetX,Y, \u00af\u00b5,\u2206+,\u2206\u2212be as given in Theo- rem 1. Also, let \u2206+T\u00af\u00b5\u2265\u2212(1\u2212\u03b4) 2\u2225\u00af\u00b5\u22252, for some small constant\u03b4>0,\u2225\u00af\u00b5\u22252\u22654 (ln 0.1)\u03b4, and\u2225\u2206+\u22252\u22640.5. Then, RwithW=IandZ= [e1,e2]is a strongly convex func- tion ofBwith condition number bounded by 20. Note that the initialization assumptions are much more strict here, but strong convexity with bounded condition number provides signi\ufb01cantly faster convergence to op- tima. Moreover, this theorem also justi\ufb01es our IHT based method. Using standard tail bounds, it is easy to show that ifngrows linearly with sBrather thand, the condi- tion number bound still holds over sparse set of vectors, i.e., for sparse \u00b5+,\u00b5\u2212and sparse b+,b\u2212. Using this re- stricted strong convexity with (Jain et al., 2014) guaranteesProtoNN: kNN for Resource-scarce Devices Figure 1. Model size (kB, X-axis) vs Accuracy (%, Y-axis): com- parison of ProtoNN against existing compression algorithms on various datasets. The left two columns show the plots for binary datasets and the right most column shows the plots for multiclass datasets. For small model size, ProtoNN is signi\ufb01cantly more ac- curate than baselines. that with just O(sBlogd)samples, our method will con- verge to a small ball around sparse \u00b5+in polynomial time. We skip these standard details as they are orthogonal to the main point of this analysis section. 6. Experiments In this section we present the performance of ProtoNN on various benchmark binary, multiclass and multilabel datasets with a goal to demonstrate the following aspects: a) In severely resource constrained settings where we re- quire model sizes to be less than 2kB (which occur rou- tinely for IoT devices like Arduino Uno), we outperform all state-of-the art compressed methods. b) For model sizes in the range 16\u221232kB, we achieve comparable accuracies to the best uncompressed methods. c) In multiclass and multilabel problems we achieve near state-of-the-art accuracies with an order of magnitude re- duction in model size, thus showing our approach is \ufb02exible and general enough to handle a wide variety of problems. Experimental Settings: Datasets: Table 3 in Ap- pendix 9.1 lists the binary, multiclass and multilabel datasets used in our experiments. For binary and multi- class datasets, we standardize each feature in the data to zero-mean and unit-variance. For multilabel datasets, we normalize the feature vector of each data point by project- ing it onto a unit norm ball which preserves data sparsity. Hyperparameters: In all our experiments, we \ufb01x the no. of alternating minimization iterations(T) to 150. Each such iteration does e-many epochs each over the 3 parameters, W,B, andZ. For small binary and multiclass datasets we do GD with eset to 20. For multilabel and large multi- class ( aloi) datasets, we do SGD with eset to 5, batch size to512. Kernel parameter \u03b3is computed after initializing B,W as2.5 median (D),whereDis the set of distances between prototypes and training points in the transformed space andis de\ufb01ned as D={\u2225bj\u2212Wxi\u22252}i\u2208[n],j\u2208[m]. ProtoNN vs. Uncompressed Baselines : In this experi- ment we compare the performance of ProtoNN with un- compressed baselines and demonstrate that even with com- pression, ProtoNN achieves near state-of-the-art accura- cies. We restrict the model size of ProtoNN to 16kB for binary datasets and to 64kB for multiclass datasets and don\u2019t place any constraints on the model sizes of baselines. We compare ProtoNN with: GBDT, RBF-SVM, 1-Hidden Layer Neural Network (1-hidden NN), kNN, BNC and SNC. For baselines the optimal hyper-parameters are se- lected through cross-validation. For SNC, BNC we set pro- jection dimensions to 100, 1280 respectively and compres- sion ratios to 16%, 1%. For ProtoNN, hyper-parameters are set based on the following heuristics which ensure that the model size constraints are satis\ufb01ed: Binary: \u02c6d= 10 , sZ=sB= 0.8.m= 40 ifsW= 1.0gives model larger than 16kB. Else, sW= 1.0andmis increased to reach 16 kB model. Multiclass: \u02c6d= 15 ,sZ=sB= 0.8. m= 5/class ifsW= 1.0gives model larger than 64kb. Else,mis increased to reach 64 kB model. CUReT which has 61 classes, requires smaller sZto satisfy model size constraints. We use the above parameter settings for all binary, multi- class datasets except for binary versions of usps,charac- terandeyewhich require 5-fold cross validation. Table 1 presents the results on binary datasets and Table 2 presents the results on multiclass datasets. For most of the datasets, ProtoNN gets to within 1\u22122% accuracy of the best uncom- pressed baseline with 1\u22122orders of magnitude reduction in model size. For example on character recognition , Pro- toNN is 0.5% more accurate than the best method (RBF- SVM) while getting \u2248400\u00d7compression in model size. Similarly, on letter-26 , our method is within 0.5% accuracy of RBF-SVM while getting \u22489\u00d7compression. Also note that ProtoNN with 16kB models is still able to outperform BNC, SNC on most of the datasets. ProtoNN vs. Compressed Baselines : In this experiment we compare the performance of ProtoNN with other state- of-the-art compressed methods in the 2-16kB model size regime: BudgetRF (Nan et al., 2015), Decision Jungle (Shotton et al., 2013), LDKL (Jose et al., 2013), Tree Prun- ing (Dekel et al., 2016), GBDT (Friedman, 1999), Bud- get Prune (Nan et al., 2016), SNC and NeuralNet Prun- ing (Han et al., 2016). All baselines plots are obtained via cross-validation. Figure 1 presents the memory vs. accuracy plots. Hyper-parameters of ProtoNN are set as follows: Binary:sB=sZ= 0.8. For [2,4,8,16] kB,\u02c6d= [5,5,10,15].sW,mare set using the same heuristic mentioned in the previous paragraph. Multiclass: sB= 0.8. For [16,32,64,128] kB,\u02c6d= [10,15,15,20]. sW,sZ,mare set as de\ufb01ned in the previous paragraph. ProtoNN values obtained with the above hyper-parametersProtoNN: kNN for Resource-scarce Devices Table 1. Comparison of ProtoNN with uncompressed baselines on binary datasets. Model size is computed as #parameters \u00d74bytes; sparse matrices taking an extra 4bytes for each non-zero entry, for the index. For BNC it is computed as #parameters/ 8bytes. GBDT model size is computed using the \ufb01le size on disk. Dataset ProtoNN kNN SNC BNC GBDT 1-hidden NN RBF-SVM character recognitionmodel size (kB) accuracy15.94 76.146870.3 67.28441.2 74.8770.88 70.68625 72.38314.06 72.536061.71 75.6 eyemodel size (kB) accuracy10.32 90.8214592 76.023305 87.761311.4 80.61234.37 83.166401.56 90.317937.45 93.88 mnistmodel size (kB) accuracy15.96 96.5183750 96.94153.6 95.74221.35 98.161171.87 98.363070 98.3335159.4 98.08 uspsmodel size (kB) accuracy11.625 95.677291 96.7568.8 97.1652.49 95.47234.37 95.91504 95.861659.9 96.86 wardmodel size (kB) accuracy15.94 96.0117589.8 94.98688 96.01167.04 93.841171.87 97.773914.06 92.757221.75 96.42 cifarmodel size (kB) accuracy15.94 76.3578125 73.73360 76.96144.06 73.741562.5 77.19314.06 75.963934.2 81.68 Table 2. Comparison of ProtoNN with uncompressed baselines on multiclass datasets. First number in each cell refers to the model size in kB and the second number denotes accuracy. Refer to Table 1 for details about calculation of model size. DatasetProtoNN (64kB)kNN SNC BNC GBDT1-hidden NNRBF SVM letter-2663.4 97.101237.8 95.26145.08 96.3631.95 92.520312 97.16164.06 96.38568.14 97.64 mnist-1063.4 95.88183984.4 94.344172 93.6220.46 96.685859.37 97.94652.34 98.4439083.7 97.3 usps-1063.83 94.927291.4 94.07568.8 94.7751.87 91.23390.62 94.32519.53 94.321559.6 95.4 curet-6163.14 94.4410037.5 89.81513.3 95.87146.70 91.872382.81 90.811310 95.518940.8 97.43 are reported for all datasets, except usps and character recognition which require 5-fold cross validation. ProtoNN performs signi\ufb01cantly better than the baselines on all the datasets. This is especially true in the 2kB regime, where ProtoNN is\u22655%more accurate on most of the datasets. ProtoNN on Multilabel and Large Multiclass Datasets : We now present the performance of ProtoNN on larger datasets. Here, we experimented with the following datasets: aloi dataset which is a relatively large multi- class dataset , mediamill, delicious, eurlex which are small- medium sized multilabel datasets. We set the hyper-parameters of ProtoNN as follows. \u02c6d is set to 30 for all datasets, except for eurlex for which we set it to 100. Other parameters are set as fol- lows:sW= 1,sB= 1 ,sZ= 5/Lforaloi and sZ= 2( avg. number of labels per training point )/Lfor multilabel datasets, m= 2\u00b7Lfor multilabel datasets. Foraloi, we compare ProtoNN with the following base- lines: 1vsA L2 Logistic Regression (1vsA-Logi), RBF- SVM, FastXML: a large-scale multilabel method (Prabhu & Varma, 2014), Recall Tree: a scalable method for large multiclass problems (Daume III et al., 2016). For 1vsA- Logi, Recall Tree we perform cross validation to pick the best tuning parameter. For FastXML we use the default pa- rameters. For RBF-SVM we set \u03b3to the default value 1/dand do a limited tuning of the regularization parameter. Left table of Figure 2 shows that ProtoNN (with m= 5000 ) gets to within 1%of the accuracy of RBF-SVM with just(1/50)thof its model size and 50times fewer \ufb02oating point computations per prediction. For a better compari- son of ProtoNN with FastXML, we set the number of pro- totypes (m= 1500 ) such that computations/prediction of both the methods are almost the same. We can see that Pro- toNN gets similar accuracy as FastXML but with a model size2orders of magnitude smaller than FastXML. Finally, our method has almost same prediction cost as Recall-Tree but with 10% higher accuracy and 4\u00d7smaller model size. Right table of Figure 2 presents preliminary results on mul- tilabel datasets. Here, we compare ProtoNN with SLEEC, FastXML and DiSMEC (Babbar & Sh \u00a8olkopf, 2016), which learns a 1vsA linear-SVM in a distributed fashion. Pro- toNN almost matches the performance of all baselines with huge reduction in model size. These results show that ProtoNN is very \ufb02exible and can handle a wide variety of problems very ef\ufb01ciently. SNC doesn\u2019t have such \ufb02exibility. For example, it can\u2019t be natu- rally extended to handle multilabel classi\ufb01cation problems. ProtoNN vs. BNC, SNC : In this experiment, we do a thor- ough performance comparison of ProtoNN with BNC and SNC. To show that ProtoNN learns better prototypes than BNC, SNC, we \ufb01x the projection dimension \u02c6dof all the methods and vary the number of prototypes m. To show that ProtoNN learns a better embedding, we \ufb01x mand vary \u02c6d. For BNC, which learns a binary embedding, \u02c6dis cho- sen such that the #parameters in its transformation ma- trix is 32times the #parameters in transformation ma- trices of ProtoNN, SNC. mis chosen similarly. Figure 3 presents the results from this experiment on mnist binary dataset. We use the following hyper parameters for Pro- toNN:sW= 0.1,sZ=sB= 1.0. For SNC, we hard threshold the input transformation matrix so that it has spar- sity0.1. Note that for small \u02c6dour method is as much asProtoNN: kNN for Resource-scarce Devices Figure 2. Left Table: ProtoNN vs baselines on aloi dataset. For Recall Tree we couldn\u2019t compute the avg. number of computations needed per prediction, instead we report the prediction time w.r.t 1vsA-Logi. Right Table: ProtoNN vs baselines on multilabel datasets. For SLEEC and FastXML we use the default parameters from the respective papers. Both the tables show that our method achieves similar accuracies as the baselines, but often with 1\u22122orders of magnitude compression in model size. On aloiour method is at most 2slower than 1-vs-all while RBF-SVM is 115\u00d7slower. Method Accuracy Model Size(MB)computations/prediction w.r.t 1vsA-Logi 1vsA-Logi 86.96 0.512 1 RBF-SVM 94.77 38.74 115.7 FastXML 89.86 254.53 0.752 Recall Tree 85.15 3.69 2.89* ProtoNN (m = 1500)89.6 0.315 0.792 ProtoNN (m = 5000)94.05 0.815 2.17Dataset FastXML DiSMEC SLEEC ProtoNN mediamill n= 30993 d= 120 L= 101model size P@1 P@3 P@57.64M 83.65 66.92 52.5148.48K 87.25 69.3 54.1957.95M 86.12 70.31 56.3354.8K 85.19 69.01 54.39 delicious n= 12920 d= 500 L= 983model size P@1 P@3 P@536.87M 69.41 64.2 59.831.97M 66.14 61.26 56.307.34M 67.77 61.27 56.62925.04K 68.92 63.04 58.32 eurlex n= 15539 d= 5000 L= 3993model size P@1 P@3 P@5410.8M 71.36 59.85 50.5179.86M 82.40 68.50 57.7061.74M 79.34 64.25 52.295.03M 77.74 65.01 53.98 Model Size10 20 30 40Accuracy 8486889092949698\u02c6d= 15 \u02c6d5 10 15Accuracy 65707580859095100m = 40 ProtoNN SNC BNC Figure 3. Comparison of ProtoNN with BNC, SNC on mnist bi- nary dataset with varying projection dimension \u02c6dor number of prototypes m. 20% more accurate than SNC, 5% more accurate than BNC and reaches nearly optimal accuracy for small \u02c6dorm. Remark 1. Before we conclude the section we provide some practical guidelines for hyper-parameter selection in ProtoNN. Consider the following two cases: a)SmallL(L\u2a850.1d):In this case, parameters \u02c6dand sWgovern the model size. Given a model size constraint, \ufb01xing one parameter \ufb01xes the other, so that we effectively have one hyper-parameter to cross-validate. Choosing m such that 10\u2264m/L\u226420typically gives good accuracies. b)LargeL(L\u2a860.1d):In this case, sZalso governs the model size. sZ,sWand\u02c6dcan be selected through cross- validation. If the model size allows it, increasing \u02c6dtypically helps. Fixing m/L to a reasonable value such as 3-10 for mediumL, 1-2 for large Ltypically gives good accuracies. 7. Experiments on tiny IoT devices In the previous section, we showed that ProtoNN gets better accuracies than other compressed baselines at low model size regimes. For small devices, it is also critical to study other aspects like energy consumption, which severely im- pact the effectiveness of a method in practice. In this sec- tion, we study the energy consumption and prediction time of ProtoNN model of size 2kB when deployed on an Ar- duino Uno.The Arduino Uno has an 8 bit, 16 MHz At- mega328P microcontroller, with 2kB of SRAM and 32kB Figure 4. Prediction time and energy consumed by ProtoNN (2kB) and its optimized version against baselines. The accuracy of each model is on top of its prediction time bar. of read-only \ufb02ash. We compare ProtoNN with 3 baselines (LDKL-L1, NeuralNet Pruning, L1 Logistic) on 4 binary datasets. Figure 4 presents the results from this experiment. ProtoNN shows almost the same characteristics as a simple linear model (L1-logistic) in most cases while providing signi\ufb01cantly more accurate predictions. Further optimization: The Atmega328P microcontroller supports native integer arithmetic at \u22480.1\u00b5s/operation, software-based \ufb02oating point arithmetic at \u22486\u00b5s/operation; exponentials are a further order slower. It is thus desirable to perform prediction only using integers. We implemented an integer version of ProtoNN to leverage this. We factor out a common \ufb02oat value from the parameters and round the residuals by 1-byte integers. To avoid computing the exponentials, we store a pre-computed table of approxi- mate exponential values. As can be seen in Figure 4, this optimized version of ProtoNN loses only a little accuracy, but obtains\u22482\u00d7reduction in energy and prediction cost.ProtoNN: kNN for Resource-scarce Devices References Angiulli, Fabrizio. Fast condensed nearest neighbor rule. InICML , 2005. Babbar, Rohit and Sh \u00a8olkopf, Bernhard. Dismec-distributed sparse machines for extreme multi-label classi\ufb01cation. InarXiv preprint arXiv:1609.02521, Accepted for Web Search and Data Mining Conference (WSDM) 2017 , 2016. Bentley, Jon Louis. Multidimensional binary search trees used for associative searching. Commun. ACM , 18:509\u2013 517, 1975. Beygelzimer, Alina, Kakade, Sham, and Langford, John. Cover trees for nearest neighbor. In ICML , 2006. Bhatia, Kush, Jain, Himanshu, Kar, Purushottam, Varma, Manik, and Jain, Prateek. Sparse local embeddings for extreme multi-label classi\ufb01cation. In NIPS , pp. 730\u2013738, 2015. Blumensath, Thomas and Davies, Mike E. Iterative thresh- olding for sparse approximations. Journal of Fourier Analysis and Applications , 14:629\u2013654, 2008. Cover, T. and Hart, P. Nearest neighbor pattern classi\ufb01ca- tion. IEEE Trans. Inf. Theor. , 13:21\u201327, 2006. Daume III, Hal, Karampatziakis, Nikos, Langford, John, and Mineiro, Paul. Logarithmic time one-against-some. arXiv preprint arXiv:1606.04988 , 2016. Davis, Jason V ., Kulis, Brian, Jain, Prateek, Sra, Suvrit, and Dhillon, Inderjit S. Information-theoretic metric learn- ing. In ICML , 2007. Dekel, O., Jacobbs, C., and Xiao, L. Pruning decision forests. In Personal Communications , 2016. Devi, V Susheela and Murty, M Narasimha. An incremen- tal prototype set building technique. Pattern Recogni- tion, 35, 2002. Friedman, Jerome H. Stochastic gradient boosting. Compu- tational Statistics and Data Analysis , 38:367\u2013378, 1999. Gionis, Aristides, Indyk, Piotr, Motwani, Rajeev, et al. Similarity search in high dimensions via hashing. In VLDB , volume 99, pp. 518\u2013529, 1999. Goldberger, Jacob, Roweis, Sam T., Hinton, Geoffrey E., and Salakhutdinov, Ruslan. Neighbourhood components analysis. In NIPS , 2004. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR , 2016.Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam. On it- erative hard thresholding methods for high-dimensional m-estimation. In NIPS , pp. 685\u2013693, 2014. Jose, Cijo, Goyal, Prasoon, Aggrwal, Parv, and Varma, Manik. Local deep kernel learning for ef\ufb01cient non- linear SVM prediction. In Proceedings of the 30th Inter- national Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013 , pp. 486\u2013494, 2013. Kulis, Brian and Darrell, Trevor. Learning to hash with binary reconstructive embeddings. In Advances in neural information processing systems , pp. 1042\u20131050, 2009. Kusner, Matt J., Tyree, Stephen, Weinberger, Kilian, and Agrawal, Kunal. Stochastic neighbor compression. In ICML , 2014. Liu, Wei, Wang, Jun, Ji, Rongrong, Jiang, Yu-Gang, and Chang, Shih-Fu. Supervised hashing with kernels. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on , pp. 2074\u20132081. IEEE, 2012. Mollineda, Ram \u00b4on Alberto, Ferri, Francesc J, and Vidal, Enrique. An ef\ufb01cient prototype merging strategy for the condensed 1-nn rule through class-conditional hier- archical clustering. Pattern Recognition , 35:2771\u20132782, 2002. Nan, F., Wang, J., and Saligrama, V . Feature-budgeted ran- dom forest. In ICML , 2015. Nan, F., Wang, J., and Saligrama, V . Pruning random forests for prediction on a budget. 2016. Norouzi, Mohammad, Fleet, David J, and Salakhutdinov, Ruslan R. Hamming distance metric learning. In Ad- vances in neural information processing systems , pp. 1061\u20131069, 2012. Prabhu, Yashoteja and Varma, Manik. Fastxml: A fast, accurate and stable tree-classi\ufb01er for extreme multi-label learning. In KDD , 2014. Shotton, J., Sharp, T., Kohli, P., Nowozin, S., Winn, J., and Criminisi, A. Decision jungles: Compact and rich models for classi\ufb01cation. In NIPS , 2013. Wang, Wenlin, Chen, Changyou, Chen, Wenlin, Rai, Piyush, and Carin, Lawrence. Deep metric learning with data summarization. In Joint European Confer- ence on Machine Learning and Knowledge Discovery in Databases , pp. 777\u2013794. Springer, 2016. Weinberger, Kilian Q. and Saul, Lawrence K. Distance metric learning for large margin nearest neighbor classi- \ufb01cation. J. Mach. Learn. Res. , 10:207\u2013244, 2009.ProtoNN: kNN for Resource-scarce Devices Weiss, Yair, Torralba, Antonio, and Fergus, Robert. Spec- tral hashing. In NIPS , pp. 1753\u20131760. Curran Asso- ciates, Inc, 2008. Zhong, Kai, Guo, Ruiqi, Kumar, Sanjiv, Yan, Bowei, Simcha, David, and Dhillon, Inderjit. Fast Classi\ufb01- cation with Binary Prototypes. In Singh, Aarti and Zhu, Jerry (eds.), Proceedings of the 20th International Conference on Arti\ufb01cial Intelligence and Statistics , vol- ume 54 of Proceedings of Machine Learning Research , pp. 1255\u20131263, Fort Lauderdale, FL, USA, 20\u201322 Apr 2017. PMLR. URL http://proceedings.mlr. press/v54/zhong17a.html .",
        "Interestingness": -1,
        "Feasibility": -1,
        "Novelty": -1,
        "novel": true
    },
    {
        "Name": "pl_knn",
        "Title": "PL-kNN: A Parameterless Nearest Neighbors Classifier",
        "Experiment": "Below is the full text of the paper with given title and afterwords the code for the implementations from the paper. Your goal is to change the code so it reproduces the idea. There are three levels of reproducibility that you might be able to check for: 1) Replicate everything 1:1 and just check if the obtained results are the same as in the paper; 2) Do just a slight variation, e.g. a different random seed, different datasets, etc. to verify that the results are not cherry picked; 3) Do larger variation, e.g. changes in the algorithm itself. Here comes the original paper:\narXiv:2209.12647v2  [cs.LG]  30 Sep 2022PL-kNN: A Parameterless Nearest Neighbors Classi\ufb01er Danilo Samuel Jodas Department of Computing S\u02dc ao Paulo State University, Brazil danilojodas@gmail.comLeandro Aparecido Passos, Ahsan Adeel CMI Lab School of Engineering and Informatics University of Wolverhampton, UK l.passosjunior@wlv.ac.uk, ahsan.adeel@deepci.orgJo\u02dc ao Paulo Papa Department of Computing S\u02dc ao Paulo State University, Brazil joao.papa@unesp.br Abstract \u2014Demands for minimum parameter setup in ma- chine learning models are desirable to avoid time-consumin g optimization processes. The k-Nearest Neighbors is one of the most effective and straightforward models employed in nume rous problems. Despite its well-known performance, it requires the value ofkfor speci\ufb01c data distribution, thus demanding expensive computational efforts. This paper proposes a k-Nearest Neighbors classi\ufb01er that bypasses the need to de\ufb01ne the value of k. The model computes the kvalue adaptively considering the data distribution of the training set. We compared the proposed m odel against the standard k-Nearest Neighbors classi\ufb01er and two parameterless versions from the literature. Experiments o ver 11 public datasets con\ufb01rm the robustness of the proposed appro ach, for the obtained results were similar or even better than its counterpart versions. Index Terms \u2014Machine Learning, k-Nearest Neighbors, Clas- si\ufb01cation, Clustering. I. I NTRODUCTION Data classi\ufb01cation is the most popular approach in machine learning. The paradigm comprises a set of labeled samples used to train a speci\ufb01c model, i.e., to learn intrinsic char- acteristics from data, for further classi\ufb01cation of unlabe led instances. In this context, one can refer to traditional met hods such as the Support Vector Machines [1] and Arti\ufb01cial Neural Networks [2], as well as graph-based models, such as the Optimum-Path Forest [3] and the k-Nearest Neighbors ( k- NN) [4]. k-NN is a method used for both classi\ufb01cation [5] and regression [6] purposes. It obtained notorious popular ity in the last three decades due to its competitive results in a wide variety of domains, ranging from medicine [7] to engineering [8], and sentiment analysis [9]. Although ef\ufb01c ient and straightforward, k-NN is sensitive to the proper selection of thekvalue, which may denote a stressful task to the user. A similar problem is faced by most machine learning methods and has been commonly overpassed through metaheuristic optimization algorithms [10]. Regarding k-NN, Wicaksono and Supianto [11] recently employed such approaches to model the problem of selecting an appropriate k. Ling et al. [12] proposed a distance-based strategy to choose the bestkby considering a region centered in each instance of the test set. On the other hand, Zhang and Song [13] 978-1-6654-9578-3/22/$31.00 \u00a92022 IEEEproposed a neural network-based model to predict the best k by considering a set of features extracted from each sampled dataset. Besides, Singh et al. [14] proposed a parameterles s version of the k-NN algorithm for regression purposes. Similar work presented by Desai et al. [15] also claims a parameterle ss version of k-NN for regression purposes. However, the model requires four extra hyperparameters. Ayyad et al. [5] propo sed a modi\ufb01ed version of the k-NN classi\ufb01er for gene expression cancer classi\ufb01cation. The proposed model de\ufb01nes a circle with radius rcentered at the test sample under prediction, whereris computed by two versions of their Modi\ufb01ed k- NN (MKNN) model: Smallest MKNN (SMKNN) and Largest MKNN (LMKNN), which measure the minimum and maxi- mum distance between the test sample and each class centroid of the training set, respectively. Although effective in th e context of gene data analysis, the method may still suffer from the high amount of neighbors in cases where the class centroids are distant from each other. This paper proposes the Parameterless k-Nearest Neighbors (PL-kNN) classi\ufb01er, a k-Nearest Neighbors variant that avoids selecting the proper value of kby introducing a mechanism that automatically chooses the number of neighbors that cor - rectly matches the data distribution. The proposed model is similar to SMKNN presented by Ayyad et al. [5]; however, we suggest the following two improvements: \u2022To use the median sample instead of the mean sample as the class centroid; \u2022To use a semicircle whose radius is de\ufb01ned as the distance of the test sample to the nearest class centroid. Regarding the second contribution, we want to \ufb01nd the traini ng samples assumed to be as close as possible to the nearest centroid of the test sample under prediction. This approach is ef\ufb01cient when there is a mixing of training samples of differ ent classes. In this case, the proposed model de\ufb01nes a semicircl e enclosing only the nearest samples close to the cluster assu med as the class of the test instance. The remainder of the paper is organized as follows: Section II introduces the proposed PL- kNN model. Afterward, Sec- tions III and IV present the methodology and the experimenta l results, respectively. Finally, Section V states conclusi ons and future work.II. P ROPOSED METHOD LetY={\u03c91,\u03c92,\u03c93,...,\u03c9 n}be the set of classes from the dataset, where \u03c9irepresents the ithclass. Also, let X={x1,x2,x3,...,xm}be the set of samples of the dataset represented by a feature vector denoted as xj\u2208Rd. In the supervised classi\ufb01cation approach, each sample xjis assigned to a class yj\u2208 Y such that the pair (xj,yj)is used in the subsequent training and testing of the classi\ufb01er. Forma lly speaking, this step involves partitioning the samples such that X=X1\u222aX2andX1\u2229X2=\u2205, whereX1andX2denote the training and testing sets, respectively. In the proposed method, the training set is split into n clusters that represent each class yi\u2208 Y. Further, the number of nearest neighbors kconnected to a target sample xjis adaptively de\ufb01ned according to its distance to all training samples inside the radius of the nearest cluster\u2019s centroid . LetC={c1,c2,...,cn}be the set of centroids such that cidenotes the centroid of the ithcluster, which contains all samples from X1that belong to class \u03c9i. The training stage of the proposed model is summarized as follows: 1) Cluster the dataset into nclusters; 2) For each sample xi\u2208 X1, assign it to the yth icluster; 3) For each yth icluster, compute its centroid cyias the median sample; 4) Compute the weight of each training sample xiregard- ing its centroid cyiusingW(xi,cyi) =DE(xi,cyi)\u22121, whereDE(xi,cyi)denotes the Euclidean distance be- tween sample xiand the centroid cyi; 5) Repeat steps 2-4 for all samples in X1. The training stage is similar to SMKNN, except for the class centroid computation, whose value is computed from the average of each training sample\u2019s features in the SMKNN variant. The PL-kNN training stage consists of \ufb01nding the cluster centroid and the distance weights of all training samples. T he distance is computed between each training sample xi\u2208 X1 and the centroid cyiof its cluster. Step 1 is concerned with the splitting of the dataset into nclusters according to the number of classes in Y. In Step 2, each training sample xiis assigned to the cluster of its class yi. Step 3 \ufb01nds the samples assumed to be the center of each cluster by computing its median feature vector. This approach is more effective than using the average due to the following reasons: i) the averag e is not a valid instance of the cluster, although it is assumed to be located as close as possible to its center, and ii) the aver age is more sensitive to outliers, and consequently, the result ing value may differ from the data distribution center. In contr ast, the median is the sample in the middle of the data distributio n, which reduces the effect of instances distant from the dense region of the cluster. Step 4 is de\ufb01ned to assign a weight for a ll instances inside the cluster. The more distant from the cent roid, the smaller the weight of the training sample. Samples dista nt from the cluster will have less impact as a neighbor of a test sample.The following steps are performed for each testing sample s: 1) Calculate the Manhattan distances DM(s,ci)between sand all cluster\u2019s centroids; 2) Get the centroid c\u22c6with the smallest distance; 3) De\ufb01ne a circle with radius DM(s,c\u22c6)around the test samples; 4) Compute the angle \u03b8betweensandc\u22c6; 5) LetT={t1,t2,...,tz}be the set of all training samplestiwith an angle \u03b8ibetween \u221290oand+90o considering the vector connecting sandc\u22c6(dark gray area inside the circle in Figure 1). The idea is to pick only the samples inside the semi-circle formed between sand the cluster centroid c\u22c6; 6) Determine the \ufb01nal class of sas the class with the higher frequency among all training samples ti\u2208 T selected in the previous step (see Equations 1 and 2 below). We want to \ufb01nd the nearest neighbors of the test sample s that signi\ufb01cantly impact the prediction of its \ufb01nal class. T his approach performs similarly to SMKNN, except for the step that chooses the instances that fall inside the circle enclo sing s. Figure 1 depicts the aforementioned idea. Fig. 1. Proposed approach to select the samples inside the se micircle that surrounds the test sample s. The dimmest area is the semicircle representation proposed for our approach. The dashed line is the distance be tweensand the nearest class centroid c\u22c6. The solid blue solid depicts the selected nearest samplet1, while the solid red line depicts a sample athat is not included in the class prediction of s. As illustrated in Figure 1, instead of picking all samples in - side the circle, we want to \ufb01nd the ones assumed to be as close as possible to the cluster of the centroid c\u22c6. Furthermore, since Euclidean distance is sensitive to high-dimensional space s, we use the Manhattan distance to avoid intensifying features w ith large differences in such scenarios. Step 6 of the algorithm above regards the \ufb01nal prediction of the test sample saccording to the following equations: pi(s) =\u2211 \u2200t\u2208T \u2227\u03bb(t)=\u03c9iDM(tj,s)\u22121\u2217W(tj,ci) \u2211n k=1pk(s),\u2200i\u2208 Y, (1) andys= argmax i(pi),\u2200i= 1,2,...,n, (2) whereysis the predicted class of s,\u03bb(t)outputs the true label of sample t, andpi(s)stands for the probability of sample sbelonging to class \u03c9i. We want to penalize the neighbor\u2019s samples of swith the farthest distance to their cluster centroid. Those samples will have less impact on the \ufb01nal prediction ofssince they are distant from their correct class group, representing possible outliers. III. M ETHODOLOGY This section presents the datasets used in this study and the setup of the experiments. A. Datasets The experiments were performed over 11 public datasets from the UCI Machine Learning1repository. The datasets include binary and multiclass labels, variation in the numb er of features, and numerical features only. The latter aspect is more concerned with avoiding the encoding of categorical featur es. The description of the datasets is presented in Table I. TABLE I DESCRIPTION OF THE DATASETS USED IN THE EXPERIMENTS. DatasetDescription Samples Features Classes Blood Transfusion (BT) 748 4 2 Breast Cancer Diagnostic (BCD) 569 30 2 Breast Cancer Original (BCO) 699 10 2 Forest Type (FT) 523 27 4 HCV data (HCV) 615 13 5 Indian Liver (IL) 583 10 2 Mammographic Mass (MM) 961 6 2 Somerville Hapiness (SH) 143 6 2 SPECT Heart (SPTH) 267 44 2 Urban Land Cover (ULC) 168 148 9 Wine (WN) 178 13 3 B. Experimental Setup The PL-kNN model2and all baselines were implemented using Python 3.6. We rely on Algorithm 1 presented in Ayyad et al. [5] to develop the source code of SMKNN and LMKNN. The datasets were divided into 20 folds of training, validation, and testing sets considering a proportion of 70 %, 15%, and 15%, respectively. Apart from the statistical anal ysis between the baselines and the proposed classi\ufb01er\u2019s results , the splitting strategy also enables the optimization of the kvalue employed by the standard k-NN classi\ufb01er using the training and validation sets. The best kwas computed from a range between 1 and 50 to \ufb01nd the value that maximizes the accuracy over the validation set. Also, we used the average accuracy and F1-Score to assess the PL- kNN effectiveness. Finally, the Wilcoxon signed-rank [16] test with 5% of signi\ufb01cance was employed to evaluate the statistical similarity among P L- kNN and the baselines over each dataset. Besides, a post hoc 1https://archive.ics.uci.edu/ml/index.php 2Source code available at https://github.com/danilojodas /PL-kNN.gitanalysis was also conducted using the Nemenyi test [17] with \u03b1= 0.05to expose the critical difference (CD) among all techniques. IV. E XPERIMENTS This section compares the PL- kNN with k-NN, SMKNN, and LMKNN. Despite intending the gene classi\ufb01cation task, SMKNN and LMKNN are easily adaptable to other contexts due to the cluster analysis, which is intrinsic to any data distribution. Moreover, both techniques work similarly to PL- kNN, particularly the SMKNN variant, which is the base of our model, thus constituting a reasonable comparison in pub lic datasets. For the sake of comparison, the best value for the k-NN model was con\ufb01gured using the optimization step described in Section III. Notice the most accurate average F1-Score is in bold, while similar results according to the Wilcoxon signed-rank test with 5%of signi\ufb01cance are underlined for all classi\ufb01ers. The F1-Score was particularly preferable t o assess the model effectiveness because of the imbalanced cl ass distribution of some datasets used in the experiments, such as the Forest Type, HCV data, and SPECT Heart. Table II presents the average results where the proposed model obtained the best F1-Score in seven out of eleven datasets. The proposed model showed similar and higher F1- Score values for the Blood Transfusion, Forest Type, HCV , Indian Liver, Mammographic, Somerville Happiness, SPECT Heart, Urban Land Cover, and Wine datasets. Even when the proposed model showed inferior results, the average metric s were almost equivalent as observed in the Forest Type and Mammographic Mass datasets. Furthermore, the proposed method surpassed the effectiveness obtained by SMKNN and LMKNN in cases where k-NN showed the best F1-Score and the results were statistically different. Notice such beha vior presented by Breast Cancer Diagnostic and Breast Cancer Original datasets. Besides, it is worth noting the low perfo r- mance of LMKNN over some datasets such as Forest Type, HCV , Indian Liver, and Urban Land Cover, which probably relates to its mechanism that assumes a more signi\ufb01cant number of neighbors while computing the radius of the circle . Besides the Wilcoxon signed-rank test, we also employ the Nemenyi test to provide an overall statistical analysis. Th e method examines the critical difference among all techniqu es to plot the method\u2019s average rank in a horizontal bar (see Figure 2). Notice lower ranks denote better performance, and the methods connected are similar in terms of statistica l signi\ufb01cance. One can notice the best result attained by the P L- kNN model. Also, the proposed approach achieved statistical similarity and better performance compared to the baseline techniques. V. C ONCLUSIONS AND FUTURE WORKS This paper presented PL- kNN, a novel approach for au- tomatically determining the number of nearest neighbors fo r thek-NN classi\ufb01er. Experiments over 11 datasets showed the competitive results obtained from the proposed modelTABLE II AVERAGE RESULTS OBTAINED BY EACH CLASSIFIER. Dataset MethodMeasure Accuracy F1-Score BTk-NN 0.7844 \u00b1 0.0270 0.3806 \u00b1 0.0934 LMKNN 0.7594 \u00b1 0.0250 0.3192 \u00b1 0.0786 SMKNN 0.7585 \u00b1 0.0319 0.4217 \u00b10.0944 PL-kNN 0.7121 \u00b1 0.0499 0.4439 \u00b10.0848 BCDk-NN 0.9553 \u00b1 0.0254 0.9373 \u00b10.0371 LMKNN 0.9076 \u00b1 0.0221 0.8595 \u00b1 0.0376 SMKNN 0.9247 \u00b1 0.0212 0.8898 \u00b1 0.0338 PL-kNN 0.9406 \u00b1 0.0176 0.9153 \u00b1 0.0266 BCOk-NN 0.9648 \u00b1 0.0195 0.9479 \u00b10.0300 LMKNN 0.7743 \u00b1 0.0278 0.5073 \u00b1 0.0896 SMKNN 0.9481 \u00b1 0.0196 0.9208 \u00b1 0.0306 PL-kNN 0.9510 \u00b1 0.0198 0.9255 \u00b1 0.0310 FTk-NN 0.8692 \u00b1 0.0554 0.8616 \u00b1 0.0614 LMKNN 0.5256 \u00b1 0.0290 0.3014 \u00b1 0.0219 SMKNN 0.8513 \u00b1 0.0437 0.8397 \u00b1 0.0557 PL-kNN 0.8538 \u00b1 0.0424 0.8494 \u00b10.0494 HCVk-NN 0.9136 \u00b1 0.0170 0.4839 \u00b10.1036 LMKNN 0.8696 \u00b1 0.0000 0.1860 \u00b1 0.0000 SMKNN 0.9071 \u00b1 0.0199 0.4843 \u00b10.1245 PL-kNN 0.9092 \u00b1 0.0147 0.4985 \u00b1 0.1197 ILk-NN 0.6897 \u00b1 0.0378 0.1958 \u00b1 0.1020 LMKNN 0.7126 \u00b1 0.0178 0.0723 \u00b1 0.0643 SMKNN 0.7121 \u00b1 0.0361 0.2558 \u00b1 0.0886 PL-kNN 0.7121 \u00b1 0.0294 0.3336 \u00b1 0.0635 MMk-NN 0.8045 \u00b1 0.0330 0.7855 \u00b1 0.0380 LMKNN 0.7740 \u00b1 0.0276 0.7627 \u00b1 0.0276 SMKNN 0.7823 \u00b1 0.0282 0.7666 \u00b1 0.0294 PL-kNN 0.7785 \u00b1 0.0302 0.7689 \u00b10.0301 SHk-NN 0.5476 \u00b1 0.1190 0.4667 \u00b10.1844 LMKNN 0.6119 \u00b1 0.0695 0.5106 \u00b10.1338 SMKNN 0.5667 \u00b1 0.0877 0.4601 \u00b10.1212 PL-kNN 0.6167 \u00b1 0.0714 0.5177 \u00b1 0.1266 SPTHk-NN 0.8087 \u00b1 0.0483 0.3797 \u00b1 0.1890 LMKNN 0.7587 \u00b1 0.0582 0.5150 \u00b10.1033 SMKNN 0.7163 \u00b1 0.0644 0.4665 \u00b1 0.0905 PL-kNN 0.7675 \u00b1 0.0507 0.5320 \u00b1 0.0702 ULCk-NN 0.7782 \u00b1 0.0265 0.7671 \u00b1 0.0351 LMKNN 0.5515 \u00b1 0.0390 0.3470 \u00b1 0.0375 SMKNN 0.7728 \u00b1 0.0359 0.7654 \u00b1 0.0393 PL-kNN 0.8005 \u00b1 0.0377 0.7946 \u00b1 0.0402 WNk-NN 0.9519 \u00b1 0.0424 0.9530 \u00b10.0424 LMKNN 0.9481 \u00b1 0.0429 0.9508 \u00b10.0421 SMKNN 0.9537 \u00b1 0.0368 0.9545 \u00b10.0370 PL-kNN 0.9630 \u00b1 0.0310 0.9640 \u00b10.0308 1 2 3 4 LMKNN SMKNN k-NNPL-kNNCD Fig. 2. Nemenyi test computed for all techniques. according to statistical analysis applied to all the baseli nes used for comparison. Besides, the Nemenyi test also con\ufb01rms the statistical similarity of the PL- kNN results with the ones obtained by the standard k-NN classi\ufb01er con\ufb01gured with the bestkvalue. Regarding future studies, we intend to identify the regions in which the entire circle may be necessary to provid e more neighboring samples to increase the prediction accura cy. Furthermore, we also plan to extend PL- kNN to regression analysis.ACKNOWLEDGMENT The authors are grateful to FAPESP grants #2013/07375- 0, #2014/12236-1, #2017/02286-0, #2018/21934-5, #2019/07665-4, and #2019/18287-0, Engineering and Physic al Sciences Research Council (EPSRC) grant EP/T021063/1, CNPq grants #307066/2017-7, and #427968/2018-6, and Petrobras grant #2017/00285-6. REFERENCES [1] M. V . Menezes, L. C. Torres, and A. P. Braga, \u201cWidth optimi zation of RBF kernels for binary classi\ufb01cation of support vector ma chines: A density estimation-based approach,\u201d Pattern Recognition Letters , vol. 128, pp. 1\u20137, 2019. [2] K. M. C. Mohammed, S. SrinivasKumar, and G. Prasad, \u201cDefe ctive texture classi\ufb01cation using optimized neural network stru cture,\u201d Pattern Recognition Letters , 2020. [3] J. P. Papa, A. X. Falc\u02dc ao, and C. T. N. Suzuki, \u201cSupervised pattern classi\ufb01cation based on optimum-path forest,\u201d International Journal of Imaging Systems and Technology , vol. 19, no. 2, pp. 120\u2013131, 2009. [4] J. L. Bentley, \u201cMultidimensional binary search trees us ed for associative searching,\u201d Communications of the ACM , vol. 18, no. 9, p. 509\u2013517, 1975. [5] S. M. Ayyad, A. I. Saleh, and L. M. Labib, \u201cGene expression cancer classi\ufb01cation using modi\ufb01ed K-Nearest Neighbors techniqu e,\u201dBioSys- tems, vol. 176, pp. 41\u201351, 2019. [6] K. J. Luken, R. P. Norris, and L. A. Park, \u201cPreliminary Res ults of Using k-nearest-neighbor Regression to Estimate the Redsh ift of Radio- selected Data Sets,\u201d Publications of the Astronomical Society of the Paci\ufb01c , vol. 131, no. 1004, p. 108003, 2019. [7] L. Zhong, L. Lin, Z. Lu, Y . Wu, Z. Lu, M. Huang, W. Yang, and Q. Feng, \u201cPredict CT image from MRI data using KNN-regressio n with learned local descriptors,\u201d in 2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI) . IEEE, 2016, pp. 743\u2013746. [8] M. Farshad and J. Sadeh, \u201cAccurate single-phase fault-l ocation method for transmission lines based on k-nearest neighbor algorit hm using one- end voltage,\u201d IEEE Transactions on Power Delivery , vol. 27, no. 4, pp. 2360\u20132367, 2012. [9] M. Murugappan, \u201cHuman emotion classi\ufb01cation using wave let transform and KNN,\u201d in 2011 International Conference on Pattern Analysis and Intelligence Robotics , vol. 1. IEEE, 2011, pp. 148\u2013153. [10] L. A. Passos and J. P. Papa, \u201cA metaheuristic-driven app roach to \ufb01ne- tune deep boltzmann machines,\u201d Applied Soft Computing , p. 105717, 2019. [11] A. S. Wicaksono and A. A. Supianto, \u201cHyper Parameter Opt imization using Genetic Algorithm on Machine Learning Methods for Onl ine News Popularity Prediction,\u201d International Journal of Advanced Com- puter Science and Applications , vol. 9, no. 12, pp. 263\u2013267, 2018. [12] P. Ling, D. Gao, X. Zhou, Z. Huang, and X. Rong, \u201cImprove t he diagnosis of atrial hypertrophy with the local discriminat ive support vector machine,\u201d Bio-Medical Materials and Engineering , vol. 26, no. s1, pp. S1813\u2013S1820, 2015. [13] X. Zhang and Q. Song, \u201cPredicting the number of nearest n eighbors for the k-nn classi\ufb01cation algorithm,\u201d Intelligent Data Analysis , vol. 18, no. 3, pp. 449\u2013464, 2014. [14] H. Singh, A. Desai, and V . Pudi, \u201cPAGER: Parameterless, Accurate, Generic, Ef\ufb01cient kNN-based Regression,\u201d in International Conference on Database and Expert Systems Applications . Springer, 2010, pp. 168\u2013176. [15] A. Desai, H. Singh, and V . Pudi, \u201cGEAR: Generic, Ef\ufb01cien t, Accurate kNN-based Regression,\u201d in International Conference on Knowledge Discovery and Information Retrieval (KDIR) , 2010, pp. 1\u201313. [16] F. Wilcoxon, \u201cIndividual comparisons by ranking metho ds,\u201dBiometrics Bulletin , vol. 1, no. 6, pp. 80\u201383, 1945. [17] P. Nemenyi, Distribution-free Multiple Comparisons . Princeton Uni- versity, 1963.\nHere comes the code of pl_nn.py:\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n'''\nX,y = make_classification(n_samples=50,\n                             n_features=2,\n                             n_classes=3,\n                             n_repeated=0,\n                             n_informative=2,\n                             n_clusters_per_class=1,\n                             class_sep=1.0,\n                             n_redundant=0,random_state=42)\n'''\n\nclass PlNearestNeighbors:\n    \"\"\"Class that implements the parameterless version of the k-Nearest Neighbors algorithm.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Constructor for property initialization.\n        \n        Args:\n            None.\n        \n        Returns:\n            None.\n        \"\"\"\n\n        self.X_train = None\n        self.y_train = None\n        self.classes = None\n        self.centers = None\n        self.k = None\n        self.nearest_neighbors = None\n        \n    def __get_angle_between_three_points(self, pointA, pointB, pointC):\n        \"\"\"Computes the angle between three points.\n\n          A\n          |\n          |\n          |_\n        B |.|___________C\n\n        theta((pointB,pointA),(pointB,pointC)\n        \n        Code adapted from:\n        https://manivannan-ai.medium.com/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd        \n\n        Args:\n            pointA (array): A n-dimensional array that represents the ending point of (pointB,pointA).\n            pointB (array): A n-dimensional array that represents the reference point of the angle between the two vectors.\n            pointC (array): A n-dimensional array that represents the ending point of (pointB,pointC).\n\n        Returns:\n            float: Angle in degrees.\n        \"\"\"\n        \n        ba = pointA - pointB\n        bc = pointC - pointB\n        \n        try:\n            cosine_angle = np.arccos(np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc)))\n        except:\n            cosine_angle = 0\n        \n        return np.degrees(cosine_angle)\n    \n    def __get_distances(self, X, Y, check_same_idx=True):\n        \"\"\"Function that computes the distance matrix from the samples in X and Y.\n\n        Args:\n            X (array): A MxN array.\n            Y (array): A KxZ array.\n            check_same_idx (bool, optional): If True, the diagonal of the distance matrix is assigned zero. Defaults to True.\n\n        Returns:\n            array: A MxK array with the distance between each element from X to all elements of Y.\n        \"\"\"\n        \n        distances = np.zeros((X.shape[0], Y.shape[0]))\n        \n        for i in range(X.shape[0]):\n            p = X[i]\n            for j in range(Y.shape[0]):\n                if (check_same_idx and i == j):\n                    continue\n                \n                #ed = np.linalg.norm(Y[j]-p)                \n                ed = np.sum(np.abs(Y[j]-p))\n                distances[i][j] = ed\n        \n        return distances\n    \n    def __get_geometric_median(self, X):\n        \"\"\"Seeks the sample whose distance to all the others in dataset is the lowest.\n        \n        The sample is considered to be center of the instances in the dataset.\n\n        Args:\n            X (array): A MxN dimensional array with the samples.\n\n        Returns:\n            array: A 1xN dimensinal array that represents the center of the dataset.\n        \"\"\"\n\n        distances = self.__get_distances(X, X)\n        \n        min_dist = np.sum(distances[0])\n        idx = 0\n        \n        for i in range(1, len(distances)):\n            if (np.sum(distances[i]) < min_dist):\n                min_dist = np.sum(distances[i])\n                idx = i\n        \n        return X[idx]\n            \n    def fit(self, X, y):\n        \"\"\"Computes the center of the classes and the weights of the samples.\n\n        Args:\n            X (array): A MxN dimensional array with the samples of the training set.\n            y (array): A Mx1 dimensional array with the labels of each sample in X.\n        \n        Returns:\n            None\n        \"\"\"\n        \n        self.X_train = np.copy(X)\n        \n        # Adding to more columns to the X_train array to store the labels (y) and the weights of each training sample to the center of the class, respectively\n        self.X_train = np.append(self.X_train,np.zeros((X.shape[0],2)),axis=1)\n        self.X_train[:,-2] = np.copy(y)\n        \n        # Auxiliary variables to store the classes and their respective centers\n        classes = np.unique(y)\n        centers = []\n        \n        # Auxiliary variable to store the weigths of the samples (with respect to the class centers)\n        w = []\n        \n        for c in classes:\n            indices = np.where(y==c)[0]\n            X_ = X[indices,:]\n            \n            # Getting the geometric median\n            #center = self.__get_geometric_median(X_)\n            #center = np.mean(X_,axis=0)\n            center = np.median(X_,axis=0)\n            centers.append(center)\n            \n            # Getting the weights of each sample\n            w = []\n            for s in X_:\n                w.append(1 / (np.linalg.norm(s-center)+0.0001))\n                # try:\n                #     w.append(1 / math.sqrt((s[0]-center[0])**2 + (s[1]-center[1])**2))\n                # except:\n                #     w.append(1)\n            \n            # Adding the class weights to the last column of the X_train array\n            self.X_train[indices,-1] = np.array(w)\n        \n        self.centers = np.vstack(centers)\n        self.classes = classes\n    \n    def predict(self, X):\n        \"\"\"Predicts the labels of each sample in test set X.\n\n        Args:\n            X (array): A MxN array with the samples of the test set.\n\n        Returns:\n            array: A Mx1 array with the predicted labels.\n        \"\"\"\n\n        y_pred = []\n        \n        i = 0\n        for t in X:\n            t = np.expand_dims(t,axis=0)\n            \n            # Distance of the test sample to all centers of class\n            distances_center = self.__get_distances(t,self.centers,False).flatten()\n            \n            # Class center with the minimum distance to the test sample\n            center_min = self.centers[np.argmin(distances_center)]\n            \n            # Distance of the test sample to all instances of the training set\n            distances = self.__get_distances(t,self.X_train[:,:-2],False).flatten()\n            \n            # Getting the minimum distance (test sample to the class centers)\n            ed = distances_center[np.argmin(distances_center)]\n            \n            # Getting the nearest neighbors, i.e., all training instances whose distances are less than the distances\n            idx_min = np.where(distances <= ed)\n            nearest_neighbors = self.X_train[idx_min]\n            \n            # Calculating the angle between the test sample and the nearest neighbors\n            angles = []\n            for n in nearest_neighbors:\n                angles.append(self.__get_angle_between_three_points(center_min, t[0], n[:-2]))\n            \n            angles = np.nan_to_num(angles)\n            \n            # Concatenating the distances of the test sample to the nearest neighbors of the training set\n            nearest_neighbors = np.concatenate((nearest_neighbors,distances[idx_min].reshape(-1,1)),axis=1)\n            \n            # Getting the neighbors inside the semi-circle\n            nearest_neighbors = nearest_neighbors[np.abs(angles) <= 90]\n            \n            # Determining the final class based on the nearest neighbors\n            if (len(nearest_neighbors) == 0):\n                final_class = self.classes[np.argmin(distances_center)]\n            elif (len(np.unique(nearest_neighbors[:,-3])) == 1):\n                final_class = np.unique(nearest_neighbors[:,-3]).astype(int)\n            else:\n                # Finding the classes of the nearest neighbors                \n                cl = np.unique(nearest_neighbors[:,-3]).astype(int)\n                classes = dict.fromkeys(cl,0)\n                \n                # Weighted sum considering the considering the distances and weights of the training instances\n                for n in nearest_neighbors:\n                    c = int(n[-3])\n                    classes[c]+=(1/n[-1]) * n[-2]\n                \n                final_class = max(classes,key=lambda x : classes[x])\n            \n            y_pred.append(final_class)\n        \n        y_pred = np.vstack(y_pred)\n        return y_pred\n    \n    def plot_neighbors(self,sample):\n        \"\"\"Function designed to visualize the nearest neighbors selected for the test sample.\n\n        Args:\n            sample (array): A 2D array with the values of the test sample.\n\n        Raises:\n            SystemExit: Thrown if the model was not trained yet.\n        \n        Returns:\n            None.\n        \"\"\"\n        \n        if (self.centers is None):\n            raise SystemExit('The model was not fitted yet!')\n        \n        sample = np.expand_dims(sample,axis=0)\n        X_train = self.X_train[:,:-2]\n        y_train = self.X_train[:,-2]\n        \n        # Applying PCA to reduce dimensionality\n        pca = PCA(n_components=2)\n        pca.fit(X_train)\n        X_train = pca.fit_transform(X_train)\n        sample = pca.transform(sample)\n        centers = pca.transform(self.centers)\n        \n        # Distance of the test sample to all centers of class\n        distances_center = self.__get_distances(sample,centers,False).flatten()\n        \n        # Class center with the minimum distance to the test sample\n        #center_min = self.centers[np.argmin(distances_center)]\n        center_min = centers[np.argmin(distances_center)]\n        \n        # Distance of the test sample to all instances of the training set\n        distances = self.__get_distances(sample,X_train,False).flatten()\n        \n        # Getting the minimum distance (test sample to the class centers)\n        ed = distances_center[np.argmin(distances_center)]\n        \n        # Getting the nearest neighbors, i.e., all training instances whose distances are less than the distances\n        idx_min = np.where(distances <= ed)\n        #nearest_neighbors = self.X_train[idx_min]\n        nearest_neighbors = X_train[idx_min]\n        \n        # Calculating the angle between the test sample and the nearest neighbors in the PCA space\n        angles = []\n        for n in nearest_neighbors:\n            angles.append(self.__get_angle_between_three_points(center_min, sample[0], n))\n        \n        angles = np.nan_to_num(angles)\n\n        # Concatenating the distances of the test sample to the nearest neighbors of the training set\n        nearest_neighbors = np.concatenate((nearest_neighbors,distances[idx_min].reshape(-1,1)),axis=1)\n        \n        # Getting the neighbors inside the semi-circle\n        nearest_neighbors = nearest_neighbors[np.abs(angles) <= 90]\n        \n        # Plot the nearest center and neighbors inside the semi-circle\n        fig, ax = plt.subplots()\n        # Plot the samples of the training set\n        for i in range(len(np.unique(y_train))):\n            ax.scatter(X_train[y_train==i,0],X_train[y_train==i,1],label=i)\n        \n        # Plot the test sample\n        ax.scatter(sample[:,0],sample[:,1],c='k')\n        \n        # Plot the nearest neighbors inside the semi-circle\n        ax.scatter(nearest_neighbors[:,0],nearest_neighbors[:,1],color=[],edgecolors='k')        \n        # Plot the nearest center\n        ax.scatter(center_min[0],center_min[1],edgecolors='k')\n        \n        # Draw the representation of the circle\n        cir = plt.Circle((sample[:,0], sample[:,1]), ed, color='k',fill=False,linestyle='--')\n        ax.add_patch(cir)\nHere comes the code for m_knn.py:\nimport math\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass MKNearestNeighbors:\n    \"\"\"Implements a k-Nearest Neighbor variant proposed for gene expression cancer classification.\n    \n    Please check the following paper to get further details regarding the method employed to implement this code:\n\n    S. M. Ayyad, A. I. Saleh, and L. M. Labib, \u201cGene expression cancer classification using modified K-Nearest Neighbors technique,\u201d BioSystems, vol. 176, pp. 41\u201351, 2019.\n    \"\"\"\n\n    def __init__(self,mode='smknn'):\n        \"\"\"Constructor to initialize the class properties.\n\n        Args:\n            mode (str, optional): Mode to get the nearest neighbors. Defaults to 'smknn'.\n\n        Raises:\n            Exception: Thrown if the mode is different from 'smknn' or 'lmknn'\n        \"\"\"\n        if (mode!='smknn' and mode!='lmknn'):\n            raise Exception('Mode parameter must be smknn or lmknn')\n            \n        self.X_train = None\n        self.y_train = None\n        self.centers = None\n        self.classes = None\n        self.k = None\n        self.nearest_neighbors = None\n        self.mode = mode\n    \n    @property\n    def mode(self) -> str:\n        \"\"\"Mode of the Modified k-NN.\n        \"\"\"\n        return self.__mode\n\n    @mode.setter\n    def mode(self,mode : str) -> None:\n        if (mode!='smknn' and mode!='lmknn'):\n            raise Exception('Mode parameter must be smknn or lmknn')\n\n        self.__mode = mode\n    \n    def __get_distances(self, X, Y, check_same_idx=True):\n        \"\"\"Computes the distance matrix from the samples in X and Y.\n\n        Args:\n            X (array): A MxN array.\n            Y (array): A KxZ array.\n            check_same_idx (bool, optional): If True, the diagonal of the distance matrix is assigned zero. Defaults to True.\n\n        Returns:\n            array: A MxK array with the distance between each element from X to all elements of Y.\n        \"\"\"\n        \n        distances = np.zeros((X.shape[0], Y.shape[0]))\n        \n        for i in range(X.shape[0]):\n            p = X[i]\n            for j in range(Y.shape[0]):\n                if (check_same_idx and i == j):\n                    continue\n                \n                ed = np.linalg.norm(Y[j]-p)                \n                distances[i][j] = ed\n        \n        return distances\n            \n    def fit(self, X, y):\n        \"\"\"Computes the center of the classes and the weights of the samples.\n\n        Args:\n            X (array): A MxN dimensional array with the samples of the training set.\n            y (array): A Mx1 dimensional array with the labels of each sample in X.\n        \n        Returns:\n            None\n        \"\"\"\n        \n        self.X_train = np.copy(X)\n        \n        # Adding to more columns to the X_train array to store the labels (y) and the weights of each training sample to the center of the class, respectively\n        self.X_train = np.append(self.X_train,np.zeros((X.shape[0],2)),axis=1)\n        self.X_train[:,-2] = np.copy(y)\n        \n        # Auxiliary variables to store the classes and their respective centers\n        classes = np.unique(y)\n        centers = []\n        \n        # Auxiliary variable to store the weigths of the samples (with respect to the class centers)\n        w = []\n        \n        for c in classes:\n            indices = np.where(y==c)[0]\n            X_ = X[indices,:]\n            \n            # Getting the center of the class\n            center = np.mean(X_,axis=0)\n            centers.append(center)\n            \n            # Getting the weights of each sample\n            w = []\n            for s in X_:\n                w.append(1 / (np.linalg.norm(s-center)+0.0001))\n            \n            # Adding the class weights to the last column of the X_train array\n            self.X_train[indices,-1] = np.array(w)\n        \n        self.centers = np.vstack(centers)\n        self.classes = classes\n    \n    def predict(self, X):\n        \"\"\"Predicts the labels of each sample in test set X.\n\n        Args:\n            X (array): A MxN array with the samples of the test set.\n\n        Returns:\n            array: A Mx1 array with the predicted labels.\n        \"\"\"\n\n        y_pred = []\n        \n        for t in X:\n            t = np.expand_dims(t,axis=0)\n            \n            # Distance of the test sample to all centers of class\n            distances_center = self.__get_distances(t,self.centers,False).flatten()\n            \n            # Distance of the test sample to all instances of the training set\n            distances = self.__get_distances(t,self.X_train[:,:-2],False).flatten()\n            \n            # Getting the distance of the test sample to the class centers\n            if (self.mode=='smknn'):\n                ed = distances_center[np.argmin(distances_center)]\n            else:\n                ed = distances_center[np.argmax(distances_center)]\n            \n            # Getting the nearest neighbors, i.e., all training instances whose distances are less than the distances\n            idx_min = np.where(distances <= ed)\n            nearest_neighbors = self.X_train[idx_min]\n            \n            # Concatenating the distances of the test sample to the nearest neighbors of the training set\n            nearest_neighbors = np.concatenate((nearest_neighbors,distances[idx_min].reshape(-1,1)),axis=1)\n                        \n            # Determining the final class based on the nearest neighbors\n            if (len(nearest_neighbors) == 0):\n                final_class = self.classes[np.argmin(distances_center)]\n            elif (len(np.unique(nearest_neighbors[:,-3])) == 1):\n                final_class = np.unique(nearest_neighbors[:,-3]).astype(int)\n            else:\n                # Finding the classes of the nearest neighbors                \n                cl = np.unique(nearest_neighbors[:,-3]).astype(int)\n                classes = dict.fromkeys(cl,0)\n                \n                # Weighted sum considering the considering the distances and weights of the training instances\n                for n in nearest_neighbors:\n                    c = int(n[-3])\n                    classes[c]+=(1/n[-1]) * n[-2]\n                \n                final_class = max(classes,key=lambda x : classes[x])\n            \n            y_pred.append(final_class)\n        \n        y_pred = np.vstack(y_pred)\n        return y_pred",
        "Interestingness": -1,
        "Feasibility": -1,
        "Novelty": -1,
        "novel": true
    }
]